{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #--------------------------------------------------------------------------------------------\n",
    "# #------------------------- 1. sensitivity to sc threshold -----------------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "# edata_medianallsub_rem = edata_medianallsub\n",
    "# nrois = 360\n",
    "# nrois_rem = nrois - len(limb_ind) \n",
    "# sub_size = MIconn_allsub.shape[2]\n",
    "# MIconn_allsub[MIconn_allsub < 0.01]=0\n",
    "# Conn_Mat = MIconn_allsub\n",
    "# thr_FC = 1\n",
    "# scmask = 1\n",
    "# thr_sc = np.linspace(0.01,1 , 20 )\n",
    "\n",
    "# corr_scthr = np.zeros((len(thr_sc)))\n",
    "# p_scthr = np.zeros((len(thr_sc)))\n",
    "\n",
    "# for i , thr in enumerate(thr_sc):\n",
    "    \n",
    "#     IC = only_IC_avg(Conn_Mat, edata_medianallsub_rem, SCconn_allsub, thr, thr_FC, sub_size, scmask, nrois_rem)\n",
    "#     corr_scthr[i] , p_scthr[i] = pcor(IC, data_avg.pet_avg)\n",
    "\n",
    "# plt.scatter(thr_sc, corr_scthr, s = 100,  c = p_scthr, cmap='coolwarm')\n",
    "# plt.colorbar()  \n",
    "# plt.ylim([0.5,0.7])\n",
    "# plt.xlabel('SC threshold')\n",
    "# plt.ylabel('Correlation IC and CMRglc (targets)')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #------------------------- 2. sensitivity to fc threshold -----------------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# plt.grid(True)\n",
    "# edata_medianallsub_rem = edata_medianallsub\n",
    "# nrois = 360\n",
    "# nrois_rem = nrois - len(limb_ind) \n",
    "# sub_size = MIconn_allsub.shape[2]\n",
    "# MIconn_allsub[MIconn_allsub < 0.0001]=0\n",
    "# Conn_Mat = MIconn_allsub\n",
    "# thr_sc = 1\n",
    "# scmask = 0\n",
    "# thr_fc = np.linspace(0.1,1 , 20 )\n",
    "\n",
    "# corr_fcthr = np.zeros((len(thr_fc)))\n",
    "# p_fcthr = np.zeros((len(thr_fc)))\n",
    "\n",
    "# for i , thr in enumerate(thr_fc):\n",
    "    \n",
    "#     IC = only_IC_avg(Conn_Mat, edata_medianallsub_rem, SCconn_allsub, thr_sc, thr, sub_size, scmask, nrois_rem)\n",
    "#     corr_fcthr[i] , p_fcthr[i] = pcor(IC, data_avg.pet_avg)\n",
    "\n",
    "# plt.scatter(thr_fc, corr_fcthr, s = 100,  c = p_fcthr, cmap='coolwarm')\n",
    "# plt.colorbar()  \n",
    "# plt.ylim([0.5,0.7])\n",
    "# plt.xlabel('MI threshold')\n",
    "# plt.ylabel('Correlation IC and CMRglc (targets)')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #-------------------------- 3. sensitivity to source numbers --------------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# corr_IC_w_source, p_IC_w_source = pcor(DATA_avg.ICallsub_w_avg, DATA_avg.degallsub_b_avg)\n",
    "\n",
    "# #sns.lmplot(data = DATA_avg , x = 'pet_avg' , y ='ICallsu_w_avg' , hue = 'yeo_7_nw'  )\n",
    "# sns.scatterplot(data = DATA_avg , x = 'degallsub_b_avg' , y ='ICallsub_w_avg' , color = 'Orange') \n",
    "# title = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_source) , float(p_IC_w_source))\n",
    "# plt.title(title , fontdict={'fontsize': 15})\n",
    "# plt.xlabel('Source Numbers')\n",
    "# plt.ylabel('IC')\n",
    "# plt.show()IC_\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #------------------- 4. sensitivity to target weighted degree -------------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "# corr_IC_w_deg, p_IC_w_deg = pcor(DATA_avg.ICallsub_w_avg, DATA_avg.degallsub_w_avg)\n",
    "# title = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_deg) , float(p_IC_w_deg))\n",
    "# #sns.lmplot(data = DATA_avg , x = 'pet_avg' , y ='ICallsu_w_avg' , hue = 'yeo_7_nw'  )\n",
    "# sns.scatterplot(data = DATA_avg , x = 'degallsub_w_avg' , y ='ICallsub_w_avg' , color ='#2fbdc6') \n",
    "# plt.title(title , fontdict={'fontsize': 15})\n",
    "# plt.xlabel('Target Degree')\n",
    "# plt.ylabel('IC')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #------------------- 5. sensitivity to region size (number of voxels) -----------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# COLOR = [227/255 , 108/255, 138/255]\n",
    "\n",
    "# num_voxels_avg = np.mean(num_voxels , axis = 1)\n",
    "# corr_IC_w_vox, p_IC_w_vox = pcor(DATA_avg.ICallsub_w_avg, num_voxels_avg)\n",
    "# sns.jointplot(x = num_voxels_avg , y = data_avg.ICallsub_w_avg ,kind=\"reg\", color = \"#8DD084\" ,marginal_kws=dict(color=GRAY, palette=[GRAY]), hue_norm =(0,3)) \n",
    "# title = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_vox) , float(p_IC_w_vox))\n",
    "# plt.title(title , fontdict={'fontsize': 15}, pad=-10)\n",
    "# plt.xlabel('ROI size (#voxels)')\n",
    "# plt.ylabel('IC')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# # Plot the scatter plots\n",
    "# ax1.scatter(data_avg.degallsub_b_avg, data_avg.ICallsub_w_avg, color=[45/255, 143/255, 143/255], label='Scatter 1')\n",
    "# ax2.scatter(data_avg.degallsub_w_avg, data_avg.ICallsub_w_avg, color=[141/255, 48/255, 101/255], label='Scatter 2')\n",
    "# ax3.scatter(num_voxels_avg, data_avg.ICallsub_w_avg, color=[111/255, 103/255, 115/255], label='Scatter 3')\n",
    "\n",
    "# # Set labels and title for each subplot\n",
    "\n",
    "# title3 = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_deg) , float(p_IC_w_deg))\n",
    "# ax1.set_title('Scatter 1')\n",
    "# ax2.set_title('Scatter 1')\n",
    "# ax3.set_title('Scatter 1')\n",
    "\n",
    " \n",
    "# ax1.set_xlabel('Source Numbers')\n",
    "# ax2.set_xlabel('Target Degree')\n",
    "# ax3.set_xlabel('ROI size (#voxels)')\n",
    "\n",
    "# ax1.set_ylabel('IC')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #-------------------------3. sensitivity to source numbers ----------------------------------\n",
    "# #---------------------- 4. sensitivity to target weighted degree ----------------------------\n",
    "# #------------------- 5. sensitivity to region size (number of voxels) -----------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "# corr_IC_w_source, p_IC_w_source = pcor(DATA_avg.pet_avg, DATA_avg.degallsub_b_avg)\n",
    "# corr_IC_w_deg, p_IC_w_deg = pcor(DATA_avg.ICallsub_w_avg, DATA_avg.degallsub_w_avg)\n",
    "# num_voxels_avg = np.mean(num_voxels , axis = 1)\n",
    "# corr_IC_w_vox, p_IC_w_vox = pcor(DATA_avg.ICallsub_w_avg, num_voxels_avg)\n",
    "\n",
    "# fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# # Plot the scatter plots\n",
    "# ax1.scatter(data_avg.degallsub_b_avg, data_avg.pet_avg, color=[45/255, 143/255, 143/255], alpha = 0.6, label='Scatter 1')\n",
    "# ax2.scatter(data_avg.degallsub_w_avg, data_avg.ICallsub_w_avg, color=[141/255, 48/255, 101/255], alpha = 0.6, label='Scatter 2')\n",
    "# ax3.scatter(num_voxels_avg, data_avg.ICallsub_w_avg,           color=[111/255, 103/255, 115/255], alpha = 0.6, label='Scatter 3')\n",
    "\n",
    "# # Set labels and title for each subplot\n",
    "# title1 = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_source) , float(p_IC_w_source))\n",
    "# title2 = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_deg) , float(p_IC_w_deg))\n",
    "# title3 = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_vox) , float(p_IC_w_vox))\n",
    "\n",
    "# ax1.set_title(title1, fontdict={'fontsize': 15})\n",
    "# ax2.set_title(title2, fontdict={'fontsize': 15})\n",
    "# ax3.set_title(title3, fontdict={'fontsize': 15})\n",
    "\n",
    "\n",
    "# ax1.set_xlabel('Source Numbers')\n",
    "# ax2.set_xlabel('Target Degree')\n",
    "# ax3.set_xlabel('ROI size (#voxels)')\n",
    "\n",
    "# ax1.set_ylabel('IC')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #------------------------------- 5. ensitivity to source numbers: ---------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "# scmask = 0\n",
    "# thr_FC = 1\n",
    "# thr_sc = 1\n",
    "# sub_size = MIconn_allsub.shape[2]\n",
    "# nrois_rem = nrois - len(limb_ind) \n",
    "# edata_medianallsub_rem = edata_medianallsub\n",
    "# MIconn_allsub[MIconn_allsub < 0.0001]=0\n",
    "# Conn_Mat = MIconn_allsub\n",
    "# #SCconn_allsub = np.ones(MIconn_allsub.shape) \n",
    "\n",
    "# source_num = np.arange(10,160,10)\n",
    "# corr_src = np.zeros((len(source_num)))\n",
    "# p_src = np.zeros((len(source_num)))\n",
    "# SCconn_allsub_th = np.zeros_like(SCconn_allsub)\n",
    "\n",
    "# for i in range(sub_size):\n",
    "#         SCconn_allsub_th[:, :, i] = threshold_proportional(SCconn_allsub[:, :, i], thr_sc)\n",
    "\n",
    "# for s , src in enumerate(source_num):\n",
    "    \n",
    "#     max_values_per_column = src\n",
    "    \n",
    "#     for i in range(sub_size):\n",
    "        \n",
    "#         SCconn = SCconn_allsub_th[:,:,i]\n",
    "#         MIconn = MIconn_allsub[:,:,i]\n",
    "\n",
    "#         SC_mask = SCconn.copy()\n",
    "#         SC_mask[SC_mask > 0] = 1\n",
    "#         MIconn_SC = np.multiply(MIconn , SC_mask)\n",
    "\n",
    "#         sorted_indices = np.argsort(-MIconn_SC, axis=0)\n",
    "#         top_indices = np.sort(sorted_indices[:max_values_per_column], axis=0)\n",
    "#         MIconn_allsub_SC[:,:,i] = np.zeros_like(MIconn_SC)\n",
    "#         MIconn_allsub_SC[top_indices, np.arange(MIconn_SC.shape[1]),i] = MIconn_SC[top_indices, np.arange(MIconn_SC.shape[1])]\n",
    "\n",
    "#     [data_avg, ICallsub_w1, ICallsub_b, degallsub_w, degallsub_b, AvgMIallsub] = IC_calculation(MIconn_allsub_SC, edata_medianallsub_rem, SCconn_allsub, thr_sc, thr_FC, sub_size, scmask, nrois_rem)\n",
    "#     corr_src[s] , p_src[s] = pcor(data_avg.ICallsub_w_avg , data_avg.pet_avg)\n",
    "\n",
    "# plt.scatter(source_num, corr_src, s = 100,  c = p_src, cmap='coolwarm')\n",
    "# plt.colorbar()  \n",
    "# plt.ylim([0.4,0.7])\n",
    "# plt.xlabel('Target\\'s source numbers')\n",
    "# plt.ylabel('Correlation IC and CMRglc (targets)')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# #test1 = np.zeros_like(MIconn_allsub)\n",
    "# #test = np.zeros_like(edata_medianallsub_rem)\n",
    "# #min_ary = np.zeros((sub_size))\n",
    "# #for i in range(sub_size):\n",
    "        \n",
    "# #    SCconn = SCconn_allsub[:,:,i]\n",
    "# #    MIconn = MIconn_allsub[:,:,i]\n",
    "\n",
    "# #    SC_mask = SCconn.copy()\n",
    "#  #   SC_mask[SC_mask > 0] = 1\n",
    "# #    MIconn_SC = np.multiply(MIconn , SC_mask)\n",
    "# #    test1[MIconn_SC > 0] = 1\n",
    "# #    min_ary[i] = np.min(np.sum(test1 , axis = 1))\n",
    "    \n",
    "# #print(min_ary)  \n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #----- 6. compare the number of connections and weighted degree for individual network ------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# degallsub_b_df = pd.DataFrame(DATA_avg.degallsub_b_avg)\n",
    "# degb = pd.concat([degallsub_b_df , net_label], axis =1)\n",
    "\n",
    "\n",
    "# for i in range(len(net_names)):\n",
    "#     degb_net = degb[degb['yeo_7_nw'] == net_names[i]].iloc[:,:-2]\n",
    "#     sns.histplot(degb_net,bins = 20)\n",
    "#     title = \"Network:{}\".format(net_names[i])\n",
    "#     plt.title(title , fontdict={'fontsize': 15})\n",
    "#     plt.show()\n",
    "    \n",
    "# degallsub_w_df = pd.DataFrame(DATA_avg.degallsub_w_avg)\n",
    "# degw = pd.concat([degallsub_w_df , net_label], axis =1)\n",
    "\n",
    "\n",
    "# for i in range(len(net_names)):\n",
    "#     degw_net = degw[degw['yeo_7_nw'] == net_names[i]].iloc[:,:-2]\n",
    "#     sns.histplot(degw_net,bins = 20)\n",
    "#     title = \"Network:{}\".format(net_names[i])\n",
    "#     plt.title(title , fontdict={'fontsize': 15})\n",
    "#     plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #*********************************************************************************************\n",
    "# #*********************************************************************************************\n",
    "# #***************************** Control analysis *********************************************\n",
    "# #*********************************************************************************************\n",
    "# #*********************************************************************************************\n",
    "\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #------------------------- 1. sensitivity to sc threshold -----------------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "# edata_medianallsub_rem = edata_medianallsub\n",
    "# nrois = 360\n",
    "# nrois_rem = nrois - len(limb_ind) \n",
    "# sub_size = MIconn_allsub.shape[2]\n",
    "# MIconn_allsub[MIconn_allsub < 0.01]=0\n",
    "# Conn_Mat = MIconn_allsub\n",
    "# thr_FC = 1\n",
    "# scmask = 1\n",
    "# thr_sc = np.linspace(0.01,1 , 20 )\n",
    "\n",
    "# corr_scthr = np.zeros((len(thr_sc)))\n",
    "# p_scthr = np.zeros((len(thr_sc)))\n",
    "\n",
    "# for i , thr in enumerate(thr_sc):\n",
    "    \n",
    "#     IC = only_IC_avg(Conn_Mat, edata_medianallsub_rem, SCconn_allsub, thr, thr_FC, sub_size, scmask, nrois_rem)\n",
    "#     corr_scthr[i] , p_scthr[i] = pcor(IC, data_avg.pet_avg)\n",
    "\n",
    "# plt.scatter(thr_sc, corr_scthr, s = 100,  c = p_scthr, cmap='coolwarm')\n",
    "# plt.colorbar()  \n",
    "# plt.ylim([0.5,0.7])\n",
    "# plt.xlabel('SC threshold')\n",
    "# plt.ylabel('Correlation IE and CMRglc (targets)')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #------------------------- 2. sensitivity to fc threshold -----------------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# plt.grid(True)\n",
    "# edata_medianallsub_rem = edata_medianallsub\n",
    "# nrois = 360\n",
    "# nrois_rem = nrois - len(limb_ind) \n",
    "# sub_size = MIconn_allsub.shape[2]\n",
    "# MIconn_allsub[MIconn_allsub < 0.0001]=0\n",
    "# Conn_Mat = MIconn_allsub\n",
    "# thr_sc = 1\n",
    "# scmask = 0\n",
    "# thr_fc = np.linspace(0.1,1 , 20 )\n",
    "\n",
    "# corr_fcthr = np.zeros((len(thr_fc)))\n",
    "# p_fcthr = np.zeros((len(thr_fc)))\n",
    "\n",
    "# for i , thr in enumerate(thr_fc):\n",
    "    \n",
    "#     IC = only_IC_avg(Conn_Mat, edata_medianallsub_rem, SCconn_allsub, thr_sc, thr, sub_size, scmask, nrois_rem)\n",
    "#     corr_fcthr[i] , p_fcthr[i] = pcor(IC, data_avg.pet_avg)\n",
    "\n",
    "# plt.scatter(thr_fc, corr_fcthr, s = 100,  c = p_fcthr, cmap='coolwarm')\n",
    "# plt.colorbar()  \n",
    "# plt.ylim([0.5,0.7])\n",
    "# plt.xlabel('MI threshold')\n",
    "# plt.ylabel('Correlation IE and CMRglc (targets)')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #-------------------------- 3. sensitivity to source numbers --------------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# corr_IC_w_source, p_IC_w_source = pcor(DATA_avg.ICallsub_w_avg, DATA_avg.degallsub_b_avg)\n",
    "\n",
    "# #sns.lmplot(data = DATA_avg , x = 'pet_avg' , y ='ICallsu_w_avg' , hue = 'yeo_7_nw'  )\n",
    "# sns.scatterplot(data = DATA_avg , x = 'degallsub_b_avg' , y ='ICallsub_w_avg' , color = 'Orange') \n",
    "# title = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_source) , float(p_IC_w_source))\n",
    "# plt.title(title , fontdict={'fontsize': 15})\n",
    "# plt.xlabel('Source Numbers')\n",
    "# plt.ylabel('IE')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #------------------- 5. sensitivity to region size (number of voxels) -----------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# COLOR = [227/255 , 108/255, 138/255]\n",
    "\n",
    "# num_voxels_avg = np.mean(num_voxels , axis = 1)\n",
    "# corr_IC_w_vox, p_IC_w_vox = pcor(DATA_avg.ICallsub_w_avg, num_voxels_avg)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(6,4))\n",
    "# g = sns.jointplot(x = num_voxels_avg , y = data_avg.ICallsub_w_avg, kind=\"reg\", color=\"#547BC9\",\n",
    "#                   marginal_kws=dict(bins=15, fill=True, color='gray'), \n",
    "#                   line_kws={'color': 'black', 'lw': 1}, \n",
    "#                   scatter_kws={'s': 85, 'alpha': 0.7, 'facecolors':\"#CBE89D\", 'edgecolors':\"#8CC62F\"}  ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# title = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_vox) , float(p_IC_w_vox))\n",
    "# g.fig.suptitle(f\"corr = {corr_IC_w_vox:.2f} , p-val = {p_IC_w_vox:.0e}\", fontsize=FONT, va='baseline', y=0.8)\n",
    "# g.set_axis_labels('Actual Target Energy', 'IE', fontsize=FONT)\n",
    "# plt.tick_params(axis='both', which='major', labelsize= FONT)  \n",
    "\n",
    "# plt.xlabel('Target size (#voxels', fontsize = FONT)\n",
    "# plt.ylabel('IE', fontsize = FONT)\n",
    "\n",
    "# ax =  g.ax_joint  \n",
    "# spine_width = 1.6  \n",
    "# for spine in ax.spines.values():\n",
    "#     spine.set_linewidth(spine_width)\n",
    "    \n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #---- same weights for one target (keep the number of connection same as the real data)------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "# SCmat =  SCconn_allsub\n",
    "# thr_sc = 0.2\n",
    "# SCmat_th = np.zeros((SCmat.shape[0], SCmat.shape[1], sub_size))\n",
    "# for i in range(sub_size):\n",
    "#     SCmat_th[:, :, i] = threshold_proportional(SCmat[:, :, i], thr_sc)\n",
    "\n",
    "# SC_mask = SCmat_th.copy()\n",
    "# SC_mask[SC_mask > 0] = 1\n",
    "\n",
    "# Conn_Mat = SC_mask\n",
    "\n",
    "# data_avg_sur2, data_single_sub_sur2 = IC_calculation(Conn_Mat, edata_medianallsub_rem, SCconn_allsub, thr_sc, thr_FC, sub_size, scmask, nrois_rem)\n",
    "# DATA_avg_sur2 = pd.concat([data_avg_sur2 , net_label], axis =1)\n",
    "\n",
    "\n",
    "\n",
    "# corr_IC_E_allsub = np.zeros((sub_size))\n",
    "# p_IC_E_allsub = np.zeros((sub_size))\n",
    "\n",
    "# for j in range(sub_size):\n",
    "                             \n",
    "#     degallsub_w_z = zscore(data_single_sub_sur2[\"degallsub_w\"][:, j] , nan_policy='omit')\n",
    "#     corr_IC_E_allsub[j], p_IC_E_allsub[j] = pcor(data_single_sub_sur2[\"ICallsub_w\"][:, j], edata_medianallsub_rem[:, j])\n",
    "#     corr_deg_w_E_allsub[j], p_deg_w_E_allsub[j] = pcor(degallsub_w_z, edata_medianallsub_rem[:, j])\n",
    "#     for i in range(len(net_names)):\n",
    "#         ind = net_label[net_label['yeo_7_nw'].str.contains(net_names[i])].index\n",
    "#         ind = ind.to_list()\n",
    "#         corr_IC_E_net_allsub[j, i], p_IC_E_net_allsub[j, i] = pcor(data_single_sub_sur2[\"ICallsub_w\"][ind, j], edata_medianallsub_rem[ind, j])\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# organg = [255/255,164/255,0/255]\n",
    "# corr_IC_w_E, p_IC_w_E = pcor(DATA_avg_sur2.ICallsub_w_avg, DATA_avg_sur2.pet_avg)\n",
    "# corr_IC_w_deg, p_IC_w_deg = pcor(DATA_avg_sur2.ICallsub_w_avg, DATA_avg_sur2.degallsub_w_avg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# g = sns.jointplot(data = DATA_avg_sur2 , x = 'pet_avg' , y ='ICallsub_w_avg' , kind=\"reg\", color=\"#C83D64\",\n",
    "#                   marginal_kws=dict(bins=15, fill=True, color='gray'), \n",
    "#                   line_kws={'color': 'black', 'lw': 1}, \n",
    "#                   scatter_kws={'s': 85, 'alpha': 0.7, 'facecolors':\"#E6E6E6\", 'edgecolors':\"#787B76\"}  ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# title = \"corr = {:.2f} , p-val = {:.0e}\".format(float(corr_IC_w_E) , float(p_IC_w_E ))\n",
    "# g.fig.suptitle(f\"corr = {corr_IC_w_E:.2f} , p-val = {p_IC_w_E :.0e}\", fontsize=FONT, va='baseline', y=0.8)\n",
    "# plt.tick_params(axis='both', which='major', labelsize= FONT)  \n",
    "\n",
    "# plt.xlabel('Energy of target', fontsize = FONT)\n",
    "# plt.ylabel('IE', fontsize = FONT)\n",
    "\n",
    "# ax =  g.ax_joint  \n",
    "# spine_width = 1.6  \n",
    "# for spine in ax.spines.values():\n",
    "#     spine.set_linewidth(spine_width)\n",
    "    \n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# #------------------ Box plot individual correlation between IC and E  -----------------------\n",
    "\n",
    "# data = pd.DataFrame({'corr': corr_IC_E_allsub, 'pval': p_IC_E_allsub})\n",
    "# fig, ax = plt.subplots(figsize=(3, 6))\n",
    "\n",
    "# violin = sns.violinplot(data=data, y='corr', ax=ax, color = '#FBE5CF')\n",
    "# sns.stripplot(data=data[data['pval'] < 0.05], y='corr', color='#E6E6E6' , size = 8, ax=ax)\n",
    "# sns.stripplot(data=data[data['pval'] >= 0.05], y='corr', color='red' , size = 8, ax=ax)\n",
    "# violin.collections[0].set_facecolor('none')\n",
    "# ax.set_ylabel('Correlation IE and target\\'s energy')\n",
    "# ax.set_ylim([-0.1,0.72])\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #--------------------------- permutation test  ----------------------------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# #1. permutation on average data (IC_avg , E_avg)\n",
    "\n",
    "# observed_correlation, observed_pvalue = pcor(DATA_avg.pet_avg, DATA_avg.ICallsub_w_avg)\n",
    "# n_permutations = 1000\n",
    "# permuted_correlations = np.zeros(n_permutations)\n",
    "\n",
    "# for perm_idx in range(n_permutations):\n",
    "#     permuted_e_avg_values = np.copy(DATA_avg.pet_avg)\n",
    "#     np.random.shuffle(permuted_e_avg_values)\n",
    "#     permuted_correlations[perm_idx], _ = pearsonr(DATA_avg.ICallsub_w_avg, permuted_e_avg_values)\n",
    "\n",
    "\n",
    "# #p_value_permutation = np.mean(np.abs(permuted_correlations) >= np.abs(observed_correlation))\n",
    "# corr_perm_avg = np.mean(permuted_correlations)\n",
    "# # Print the results\n",
    "# print(\"Permutation Test Results:\")\n",
    "# print(f\"Observed Correlation: {observed_correlation}\")\n",
    "# print(f\"Observed p-value: {observed_pvalue}\")\n",
    "# print(f\"Average perumuted correlation : {corr_perm_avg }\")\n",
    "\n",
    "\n",
    "# sns.displot(permuted_correlations, kde=True, color='#626dd4', kde_kws={'color': 'red'})\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #2. permutation on individual data (IC , E)\n",
    "# permuted_correlations = np.zeros((n_permutations, sub_size))\n",
    "# for i in range(sub_size):\n",
    "    \n",
    "#     E_sub = edata_medianallsub_rem[:,i]\n",
    "#     IC_sub = ICallsub_w[:,i]\n",
    "    \n",
    "#     for perm_idx in range(n_permutations):\n",
    "        \n",
    "#         permuted_E_values = np.copy(E_sub)\n",
    "#         np.random.shuffle(permuted_E_values)\n",
    "#         permuted_correlations[perm_idx,i], _ = pcor(IC_sub, permuted_E_values)\n",
    "\n",
    "\n",
    "# #sns.boxplot(permuted_correlations)\n",
    "# #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "# #---------------------- Ridge plot of each subject histogram  -------------------------------\n",
    "# #--------------------------------------------------------------------------------------------\n",
    "\n",
    "# num_columns = permuted_correlations.shape[1]\n",
    "# reshaped_array = np.vstack((permuted_correlations.flatten(), np.tile(np.arange(num_columns), permuted_correlations.shape[0]))).T\n",
    "# df_reshaped = pd.DataFrame(reshaped_array, columns=['Correlation Value', 'Subject'])\n",
    "\n",
    "\n",
    "# df_filtered = df_reshaped\n",
    "# sns.set_theme(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0), 'axes.linewidth':1})\n",
    "# palette = \"Spectral\"\n",
    "# g = sns.FacetGrid(df_filtered, palette=palette, row=\"Subject\", hue=\"Subject\", aspect=9, height=0.3)\n",
    "# g.map_dataframe(sns.kdeplot, x='Correlation Value', fill=True, alpha=0.8)\n",
    "# g.map_dataframe(sns.kdeplot, x='Correlation Value', color='black', linewidth= 0.7)\n",
    "# def label(x, color, label):\n",
    "#     ax = plt.gca()\n",
    "#     ax.text(0, .2, label, color='black', fontsize=10,\n",
    "#             ha=\"left\", va=\"center\", transform=ax.transAxes)\n",
    "\n",
    "# #g.map(label, \"Subject\")\n",
    "# g.fig.subplots_adjust(hspace=-.5)\n",
    "# g.set_titles(\"\")\n",
    "# #g.set(yticks=[], xlabel=\"Correlation Value\")\n",
    "# #g.despine( left=True)\n",
    "# #plt.suptitle('Netflix Originals - IMDB Scores by Language', y=0.98)\n",
    "# for ax in g.axes.flat:\n",
    "#     ax.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "# g.despine(left=True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pcor (x,y):\n",
    "    from scipy.stats import pearsonr\n",
    "    non_nan_indices = ~np.isnan(x) & ~np.isnan(y)\n",
    "    xx = x[non_nan_indices]\n",
    "    yy = y[non_nan_indices]\n",
    "    corr = pearsonr(xx,yy)\n",
    "    \n",
    "    # bad = ~np.logical_or(np.isnan(x), np.isnan(y))\n",
    "    #xx = np.compress(bad, x)  \n",
    "    #yy = np.compress(bad, y) \n",
    "    \n",
    "    corr, pvalue = pearsonr(xx, yy)  # Unpack both values here\n",
    "    return corr, pvalue\n",
    "   \n",
    "\n",
    "def clean_corr(x,y,n_mad=2):\n",
    "    x[pd.isna(x)]=x.min()#0\n",
    "    y[pd.isna(y)]=y.min()#0\n",
    "    x = x.astype(float)\n",
    "    y = y.astype(float)\n",
    "    if not isinstance(n_mad, str):\n",
    "        x_mad = stats.median_abs_deviation(x[x>x.min()],nan_policy='omit')\n",
    "        x_med = np.nanmedian(x[x>x.min()])\n",
    "        y_mad = stats.median_abs_deviation(y[y>y.min()],nan_policy='omit')\n",
    "        y_med = np.nanmedian(y[y>y.min()])\n",
    "        valid_ind = ((x<(x_med+(n_mad*x_mad))) & (x>(x_med-(n_mad*x_mad))) & (y<(y_med+(n_mad*y_mad))) & (y>(y_med-(n_mad*y_mad))) & (x>x.min()) & (y>y.min()))\n",
    "    else:\n",
    "        valid_ind = ((x>np.nanmin(x)) & (y>np.nanmin(y)))\n",
    "    \n",
    "    return stats.pearsonr(x[valid_ind], y[valid_ind])\n",
    "###>!!!!filter_plot_joint(external_datasets['expansion']['data'][:180],metric2mmp(all_avg_roi_vals,y_var,'roi_id'),xlabel='Brain expansion',ylabel=ylabel,n_mad='min')\n",
    "\n",
    "\n",
    "\n",
    "def threshold_proportional(W, p, copy=True):\n",
    "    '''\n",
    "    This function \"thresholds\" the connectivity matrix by preserving a\n",
    "    proportion p (0<p<1) of the strongest weights. All other weights, and\n",
    "    all weights on the main diagonal (self-self connections) are set to 0.\n",
    "    If copy is not set, this function will *modify W in place.*\n",
    "    Inputs: W,      weighted or binary conneccivity matrix\n",
    "            p,      proportion of weights to preserve\n",
    "                        range:  p=1 (all weights preserved) to\n",
    "                                p=0 (nco weights preserved)\n",
    "            copy,    copy W to avoid side effects, defaults to True\n",
    "    Output: W,        thresholded connectivity matrix\n",
    "    Note: The proportion of elements set to 0 is a fraction of all elements in the \n",
    "    matrix, whether or not they are already 0. That is, this function has the\n",
    "    following behavior:\n",
    "        >> x = np.random.random((10,10))\n",
    "        >> x_25 = threshold_proportional(x, .25)\n",
    "        >> np.size(np.where(x_25)) #note this double counts each nonzero element\n",
    "        46\n",
    "        >> x_125 = threshold_proportional(x, .125)\n",
    "        >> np.size(np.where(x_125))\n",
    "        22\n",
    "        >> x_test = threshold_proportional(x_25, .5)\n",
    "        >> np.size(np.where(x_test))\n",
    "        46\n",
    "    That is, the 50% thresholding of x_25 does nothing because >=50% of the elements\n",
    "    in x_25 are aleady <=0. This behavior is the same as in BCT. Be careful with matrices that are both signed and sparse.\n",
    "    '''\n",
    "    if p > 1 or p < 0:\n",
    "        raise BCTParamError('Threshold must be in range [0,1]')\n",
    "    if copy:\n",
    "        W = W.copy()\n",
    "    n = len(W)                        # number of nodes\n",
    "    np.fill_diagonal(W, 0)            # clear diagonal\n",
    "\n",
    "    if np.all(W == W.T):                # if symmetric matrix\n",
    "        W[np.tril_indices(n)] = 0        # ensure symmetry is preserved\n",
    "        ud = 2                        # halve number of removed links\n",
    "    else:\n",
    "        ud = 1\n",
    "\n",
    "    ind = np.where(W)                    # find all links\n",
    "\n",
    "    I = np.argsort(W[ind])[::-1]        # sort indices by magnitude\n",
    "\n",
    "    # number of links to be preserved\n",
    "    en = round((n * n - n) * p / ud)\n",
    "\n",
    "    W[(ind[0][I][en:], ind[1][I][en:])] = 0    # apply threshold\n",
    "\n",
    "    if ud == 2:                          # if symmetric matrix\n",
    "        W[:, :] = W + W.T                        # reconstruct symmetry\n",
    "\n",
    "    return W\n",
    "\n",
    "def only_IC_avg(FCmat, pet, SCmat, thr_sc, thr_FC, sub_size, scmask, nrois_rem):   \n",
    "    \n",
    "    SCmat_th = np.zeros((SCmat.shape[0], SCmat.shape[1], sub_size))\n",
    "    for i in range(sub_size):\n",
    "        SCmat_th[:, :, i] = threshold_proportional(SCmat[:, :, i], thr_sc)\n",
    "\n",
    "    SC_mask = SCmat_th.copy()\n",
    "    SC_mask[SC_mask > 0] = 1\n",
    "\n",
    "\n",
    "    FCmat_SC = np.multiply(FCmat , SC_mask)\n",
    "\n",
    "\n",
    "    pet_rem = pet.copy()\n",
    "\n",
    "    ICallsub_w = np.zeros((nrois_rem, sub_size))\n",
    "\n",
    "    for j in range(sub_size):\n",
    "        FCconn = FCmat_SC[:, :, j]\n",
    "        CMR2 = pet_rem[:, j]\n",
    "        FCconn_th = threshold_proportional(FCconn, thr_FC)\n",
    "        E_MAT = np.tile(CMR2, (nrois_rem, 1)) #########\n",
    "\n",
    "        \n",
    "\n",
    "        #row_sums = np.sum(FCconn_th, axis=1)  # Compute the sum of each row\n",
    "       # FCconn_thnor = FCconn_th / row_sums[:, np.newaxis]  # Divide each element by the corresponding row sum\n",
    "     \n",
    "        \n",
    "        row_sums = np.nansum(FCconn_th, axis=1)  # Compute the sum of each row\n",
    "        FCconn_thnor = np.divide(FCconn_th, row_sums[:, np.newaxis], out=np.zeros_like(FCconn_th), where=row_sums[:, np.newaxis] != 0)\n",
    "\n",
    "        # FCconn_thnor = np.zeros((nrois_rem, nrois_rem))\n",
    "       \n",
    "\n",
    "       # for r in range(nrois_rem):\n",
    "          #  for s in range(nrois_rem):\n",
    "           #     FCconn_thnor[r, s] = np.divide(FCconn_th[r, s] , np.nansum(FCconn_th[r, :]))\n",
    "                \n",
    "        \n",
    "        IC_w = np.nansum(np.multiply(FCconn_thnor , E_MAT), axis=1)\n",
    "\n",
    "        ICallsub_w[:, j] = IC_w\n",
    "\n",
    "    ICallsub_w[ICallsub_w == 0] = np.nan\n",
    "    ICallsub_w_avg = np.nanmean(ICallsub_w, axis=1)\n",
    "    \n",
    "    return ICallsub_w_avg\n",
    "\n",
    "def IC_calculation(FCmat, pet, SCmat, thr_sc, thr_FC, sub_size, scmask, nrois_rem):\n",
    "\n",
    "# ####################### temporary ##########################################\n",
    "    \n",
    "#     from scipy.ndimage import generate_binary_structure, binary_dilation\n",
    "\n",
    "\n",
    "#     nii = nib.load(f'/RAID1/jupytertmp/mi/data/MMP_in_MNI_corr_3mm.nii.gz')\n",
    "#     data = nii.get_fdata().astype(int)\n",
    "    \n",
    "#     region_labels = np.unique(data)\n",
    "#     region_labels = region_labels[region_labels != 0]\n",
    "#     num_regions = len(region_labels)\n",
    "    \n",
    "    \n",
    "#     label_to_index = {label: idx for idx, label in enumerate(region_labels)}\n",
    "    \n",
    "    \n",
    "#     connectivity_matrix = np.zeros((num_regions, num_regions), dtype=int)\n",
    "    \n",
    "    \n",
    "#     structure = generate_binary_structure(3, 1)\n",
    "    \n",
    "    \n",
    "#     for label in region_labels:\n",
    "#         region_mask = (data == label)\n",
    "#         dilated = binary_dilation(region_mask, structure=structure)\n",
    "#         border_voxels = dilated & ~region_mask\n",
    "        \n",
    "#         # Find labels of neighboring regions at the border\n",
    "#         neighbor_labels = np.unique(data[border_voxels])\n",
    "#         neighbor_labels = neighbor_labels[(neighbor_labels != 0) & (neighbor_labels != label)]\n",
    "        \n",
    "#         for neighbor in neighbor_labels:\n",
    "#             i = label_to_index[label]\n",
    "#             j = label_to_index[neighbor]\n",
    "#             connectivity_matrix[i, j] = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     connectivity_matrix = np.delete(connectivity_matrix, limb_ind, axis = 0)\n",
    "#     connectivity_matrix = np.delete(connectivity_matrix, limb_ind, axis = 1)\n",
    "    \n",
    "#     connectivity_matrix_allsub = np.stack([connectivity_matrix]*sub_size, axis=2)\n",
    "#     connectivity_matrix_allsub = ~ connectivity_matrix_allsub.astype(bool)\n",
    "\n",
    "# #################################################################\n",
    "\n",
    "\n",
    "    \n",
    "    data_single_sub = {\n",
    "        'ICallsub_w': np.zeros((nrois_rem, sub_size)),\n",
    "        'ICallsub_b': np.zeros((nrois_rem, sub_size)),\n",
    "        'degallsub_w': np.zeros((nrois_rem, sub_size)),\n",
    "        'degallsub_b': np.zeros((nrois_rem, sub_size)),\n",
    "        'AvgMIallsub': np.zeros((nrois_rem, sub_size)),\n",
    "        'eig_cenallsub': np.zeros((nrois_rem, sub_size)),\n",
    "        'btw_cenallsub': np.zeros((nrois_rem, sub_size)),\n",
    "    }\n",
    "    \n",
    "    SCmat_th = np.zeros((SCmat.shape[0], SCmat.shape[1], sub_size))\n",
    "   \n",
    "    for i in range(sub_size):\n",
    "        SCmat_th[:, :, i] = threshold_proportional(SCmat[:, :, i], thr_sc)\n",
    "\n",
    "    SC_mask = SCmat_th.copy()\n",
    "    SC_mask[SC_mask > 0] = 1\n",
    "\n",
    "    \n",
    "\n",
    "    pet_rem = pet.copy()\n",
    "\n",
    "    \n",
    "\n",
    "    for j in range(sub_size):\n",
    "        \n",
    "        FCconn = FCmat[:, :, j]\n",
    "        SCconn = SC_mask[:,:,j]\n",
    "        \n",
    "        \n",
    "        \n",
    "        FCconn_th = threshold_proportional(FCconn, thr_FC)\n",
    "        \n",
    "        # fc_neighbor_mask = connectivity_matrix_allsub[:, :, j]\n",
    "        # FCconn_th = FCconn_th * fc_neighbor_mask\n",
    "        \n",
    "        if scmask == 1:\n",
    "            FCmat_SC = np.multiply(FCconn_th , SCconn)\n",
    "        \n",
    "        else:\n",
    "            FCmat_SC = FCconn_th\n",
    "        CMR2 = pet_rem[:, j]\n",
    "        \n",
    "        #FCconn_th = FCconn\n",
    "        deg_w = np.nansum(FCmat_SC, axis=1)\n",
    "        FCconn_th_b = FCmat_SC.copy()\n",
    "        FCconn_th_b[FCconn_th_b > 0] = 1\n",
    "        deg_b = np.nansum(FCconn_th_b, axis=1)\n",
    "        E_MAT = np.tile(CMR2, (nrois_rem, 1)) #########\n",
    "        \n",
    "        \n",
    "        GFC = nx.Graph(FCmat_SC)\n",
    "        \n",
    "        #eigenvector_centrality = nx.eigenvector_centrality(GFC)\n",
    "        #eig_cen = list(eigenvector_centrality.values())\n",
    "        \n",
    "        betweenness_centrality = nx.betweenness_centrality(GFC , normalized=True, weight = 'weight')\n",
    "        btw_cen = list(betweenness_centrality.values())\n",
    "\n",
    "        #FCconn_thnor = np.zeros((nrois_rem, nrois_rem))\n",
    "        #FCconn_thnor_b = np.zeros((nrois_rem, nrois_rem))\n",
    "\n",
    "       # for r in range(nrois_rem):\n",
    "          #  for s in range(nrois_rem):\n",
    "              #  FCconn_thnor[r, s] = np.divide(FCconn_th[r, s] , np.nansum(FCconn_th[r, :]))\n",
    "               # FCconn_thnor_b[r, s] = np.divide(FCconn_th_b[r, s]  , np.nansum(FCconn_th_b[r, :]))\n",
    "                \n",
    "        row_sums = np.nansum(FCmat_SC, axis=1)  # Compute the sum of each row\n",
    "        FCconn_thnor = np.divide(FCmat_SC , row_sums[:, np.newaxis], out=np.zeros_like(FCmat_SC ), where=row_sums[:, np.newaxis] != 0)\n",
    "\n",
    "        row_sums_b = np.nansum(FCconn_th_b, axis=1)  # Compute the sum of each row\n",
    "        FCconn_thnor_b = np.divide(FCconn_th_b, row_sums_b[:, np.newaxis], out=np.zeros_like(FCconn_th_b), where=row_sums[:, np.newaxis] != 0)\n",
    "\n",
    "        IC_w = np.nansum(np.multiply(FCconn_thnor , E_MAT), axis=1)\n",
    "        IC_b = np.nansum(np.multiply(FCconn_thnor_b ,E_MAT), axis=1)\n",
    "        AvgMI = np.divide(deg_w , deg_b)\n",
    "\n",
    "        data_single_sub['ICallsub_w'][:, j] = IC_w\n",
    "        data_single_sub['ICallsub_b'][:, j] = IC_b\n",
    "        data_single_sub['degallsub_w'][:, j] = deg_w\n",
    "        data_single_sub['degallsub_b'][:, j] = deg_b\n",
    "        data_single_sub['AvgMIallsub'][:, j] = AvgMI\n",
    "        #data_single_sub['eig_cenallsub'][:, j] = eig_cen\n",
    "        data_single_sub['btw_cenallsub'][:, j] = btw_cen\n",
    "        \n",
    "\n",
    "    data_single_sub['ICallsub_w'][data_single_sub['ICallsub_w'] == 0] = np.nan\n",
    "    data_single_sub['ICallsub_b'][data_single_sub['ICallsub_b'] == 0] = np.nan\n",
    "    data_single_sub['degallsub_w'][data_single_sub['degallsub_w'] == 0] = np.nan\n",
    "    data_single_sub['degallsub_b'][data_single_sub['degallsub_b'] == 0] = np.nan\n",
    "    data_single_sub['AvgMIallsub'][data_single_sub['AvgMIallsub']== 0] = np.nan\n",
    "    #data_single_sub['eig_cenallsub'][data_single_sub['eig_cenallsub'] == 0] = np.nan\n",
    "    data_single_sub['btw_cenallsub'][data_single_sub['btw_cenallsub'] == 0] = np.nan\n",
    "    \n",
    "\n",
    "    degallsub_w_avg = np.nanmean(data_single_sub['degallsub_w'], axis=1)\n",
    "    degallsub_b_avg = np.nanmean(data_single_sub['degallsub_b'], axis=1)\n",
    "    pet_avg = np.nanmean(pet_rem, axis=1)\n",
    "    ICallsub_w_avg = np.nanmean(data_single_sub['ICallsub_w'], axis=1)\n",
    "    ICallsub_b_avg = np.nanmean(data_single_sub['ICallsub_b'], axis=1)\n",
    "    AvgMIallsub_avg = np.nanmean(data_single_sub['AvgMIallsub'], axis=1)\n",
    "    #eig_cenallsub_avg = np.nanmean(data_single_sub['eig_cenallsub'], axis=1)\n",
    "    btw_cenallsub_avg = np.nanmean(data_single_sub['btw_cenallsub'], axis=1)\n",
    "    \n",
    "    \n",
    "    data_avg = pd.DataFrame({'degallsub_w_avg' : degallsub_w_avg ,'degallsub_b_avg' : degallsub_b_avg,\n",
    "                       'pet_avg' : pet_avg , 'ICallsub_w_avg' : ICallsub_w_avg , \n",
    "                         'ICallsub_b_avg' : ICallsub_b_avg, 'AvgMIallsub_avg': AvgMIallsub_avg,  'btw_avg': btw_cenallsub_avg})#'eig_avg': eig_cenallsub_avg,\n",
    "        \n",
    "  \n",
    "    return data_avg, data_single_sub\n",
    "# def pcor (x,y):\n",
    "    \n",
    "#     bad = ~np.logical_or(np.isnan(x), np.isnan(y))\n",
    "#     xx = np.compress(bad, x)  \n",
    "#     yy = np.compress(bad, y)  \n",
    "#     corr = pearsonr(xx,yy)\n",
    "#     return corr\n",
    "\n",
    "\n",
    "def IC_within_btw_network(FCmat, pet, SCmat, thr_sc, thr_FC, sub_size, scmask, nrois_rem , net_label, net_names):\n",
    "    \n",
    "        \n",
    "    SCmat_th = np.zeros((SCmat.shape[0], SCmat.shape[1], sub_size))\n",
    "    for i in range(sub_size):\n",
    "        SCmat_th[:, :, i] = threshold_proportional(SCmat[:, :, i], thr_sc)\n",
    "\n",
    "    SC_mask = SCmat_th.copy()\n",
    "    SC_mask[SC_mask > 0] = 1\n",
    "\n",
    "    if scmask == 1:\n",
    "        FCmat_SC = np.multiply(FCmat , SC_mask)\n",
    "        \n",
    "    else:\n",
    "        FCmat_SC = FCmat\n",
    "\n",
    "    pet_rem = pet.copy()\n",
    "\n",
    "    ICallsub_wtn_w = np.zeros((nrois_rem, sub_size))\n",
    "    ICallsub_wtn_b = np.zeros((nrois_rem, sub_size))\n",
    "    \n",
    "    ICallsub_btw_w = np.zeros((nrois_rem, sub_size))\n",
    "    ICallsub_btw_b = np.zeros((nrois_rem, sub_size))\n",
    "    \n",
    "    degallsub_w = np.zeros((nrois_rem, sub_size))\n",
    "    degallsub_b = np.zeros((nrois_rem, sub_size))\n",
    "    \n",
    "    for j in range(sub_size):\n",
    "        FCconn = FCmat_SC[:, :, j]\n",
    "        CMR2 = pet_rem[:, j]\n",
    "        FCconn_th = threshold_proportional(FCconn, thr_FC)\n",
    "        #FCconn_th = FCconn\n",
    "        deg_w = np.nansum(FCconn_th, axis=1)\n",
    "        FCconn_th_b = FCconn_th.copy()\n",
    "        FCconn_th_b[FCconn_th_b > 0] = 1\n",
    "        deg_b = np.nansum(FCconn_th_b, axis=1)\n",
    "        E_MAT = np.tile(CMR2, (nrois_rem, 1)) #########\n",
    "\n",
    "        FCconn_thnor_wtn_net = np.zeros((nrois_rem, nrois_rem))\n",
    "        FCconn_thnor_b_wtn_net = np.zeros((nrois_rem, nrois_rem))\n",
    "        \n",
    "        FCconn_thnor_btw_net = np.zeros((nrois_rem, nrois_rem))\n",
    "        FCconn_thnor_b_btw_net = np.zeros((nrois_rem, nrois_rem))\n",
    "\n",
    "        IC_wtn_w = np.zeros((nrois_rem))\n",
    "        IC_wtn_b = np.zeros((nrois_rem))\n",
    "        \n",
    "        IC_btw_w = np.zeros((nrois_rem))\n",
    "        IC_btw_b = np.zeros((nrois_rem))\n",
    "        \n",
    "        for k in range(len(net_names)):\n",
    "            \n",
    "            ind = net_label[net_label['yeo_7_nw'].str.contains(net_names[k])].index\n",
    "            ind = ind.to_list()\n",
    "            complement = np.setdiff1d(np.arange(len(net_label)), ind)\n",
    "            \n",
    "            FCconn_th_wtn_net = np.zeros((nrois_rem, nrois_rem))\n",
    "            FCconn_th_wtn_net[ind , :] = FCconn_th[ind,:]\n",
    "            FCconn_th_wtn_net[: , ind] = FCconn_th_wtn_net[:,ind]\n",
    "            \n",
    "            FCconn_th_btw_net = np.zeros((nrois_rem, nrois_rem))\n",
    "            FCconn_th_btw_net[ind , :] = FCconn_th[ind , :]\n",
    "            FCconn_th_btw_net[: , complement] = FCconn_th_btw_net[: , complement]\n",
    "            \n",
    "            FCconn_th_b_wtn_net = np.zeros((nrois_rem, nrois_rem))\n",
    "            FCconn_th_b_wtn_net[ind , :] = FCconn_th[ind,:]\n",
    "            FCconn_th_b_wtn_net[: , ind] = FCconn_th_b_wtn_net[:,ind]\n",
    "            \n",
    "            FCconn_th_b_btw_net = np.zeros((nrois_rem, nrois_rem))\n",
    "            FCconn_th_b_btw_net[ind , :] = FCconn_th[ind , :]\n",
    "            FCconn_th_b_btw_net[: , complement] = FCconn_th_b_btw_net[: , complement]\n",
    "\n",
    "            \n",
    "            E_MAT_wtn_net = np.zeros((nrois_rem, nrois_rem))\n",
    "            E_MAT_wtn_net[ind , :] = E_MAT[ind,:]\n",
    "            E_MAT_wtn_net[: , ind] = E_MAT_wtn_net[:,ind]\n",
    "            \n",
    "            E_MAT_btw_net = np.zeros((nrois_rem, nrois_rem))\n",
    "            E_MAT_btw_net[ind , :] = E_MAT[ind , :]\n",
    "            E_MAT_btw_net[: , complement] = E_MAT_btw_net[: , complement]\n",
    "\n",
    "           \n",
    "            for r in ind:\n",
    "                for s in ind:\n",
    "                    \n",
    "                    FCconn_thnor_wtn_net[r, s] = np.divide(FCconn_th_wtn_net[r, s] , np.nansum(FCconn_th_wtn_net[r, :]))\n",
    "                    FCconn_thnor_b_wtn_net[r, s] = np.divide(FCconn_th_b_wtn_net[r, s]  , np.nansum(FCconn_th_b_wtn_net[r, :]))\n",
    "            \n",
    "            for r in range(len(ind)):\n",
    "                for s in range(len(complement)):\n",
    "\n",
    "                    FCconn_thnor_btw_net[r, s] = np.divide(FCconn_th_btw_net[r, s] , np.nansum(FCconn_th_btw_net[r, :]))\n",
    "                    FCconn_thnor_b_btw_net[r, s] = np.divide(FCconn_th_b_btw_net[r, s]  , np.nansum(FCconn_th_b_btw_net[r, :]))\n",
    "\n",
    "        \n",
    "\n",
    "                        \n",
    "            temp1 = np.nansum(np.multiply(FCconn_thnor_wtn_net , E_MAT_wtn_net), axis=1)\n",
    "            IC_wtn_w[ind] = temp1[ind]\n",
    "            temp2 = np.nansum(np.multiply(FCconn_thnor_b_wtn_net ,E_MAT_wtn_net), axis=1)\n",
    "            IC_wtn_b[ind] = temp2[ind]\n",
    "            \n",
    "            \n",
    "            \n",
    "            temp3 = np.nansum(np.multiply(FCconn_thnor_btw_net , E_MAT_btw_net), axis=1)\n",
    "            IC_btw_w[ind] = temp3[ind]\n",
    "            temp4 = np.nansum(np.multiply(FCconn_thnor_b_btw_net ,E_MAT_btw_net), axis=1)\n",
    "            IC_btw_b[ind] = temp4[ind]\n",
    "\n",
    "            \n",
    "            \n",
    "        ICallsub_btw_w[:, j] = IC_btw_w\n",
    "        ICallsub_btw_b[:, j] = IC_btw_b\n",
    "        \n",
    "        ICallsub_wtn_w[:, j] = IC_wtn_w\n",
    "        ICallsub_wtn_b[:, j] = IC_wtn_b\n",
    "        \n",
    "        degallsub_w[:, j] = deg_w\n",
    "        degallsub_b[:, j] = deg_b\n",
    "        \n",
    "\n",
    "    ICallsub_btw_w[ICallsub_btw_w == 0] = np.nan\n",
    "    ICallsub_btw_b[ICallsub_btw_b == 0] = np.nan\n",
    "    \n",
    "    ICallsub_wtn_w[ICallsub_wtn_w == 0] = np.nan\n",
    "    ICallsub_wtn_b[ICallsub_wtn_b == 0] = np.nan\n",
    "    \n",
    "    degallsub_w[degallsub_w == 0] = np.nan\n",
    "    degallsub_b[degallsub_b == 0] = np.nan\n",
    "    \n",
    "    degallsub_w_avg = np.nanmean(degallsub_w, axis=1)\n",
    "    degallsub_b_avg = np.nanmean(degallsub_b, axis=1)\n",
    "    pet_avg = np.nanmean(pet_rem, axis=1)\n",
    "    ICallsub_wtn_w_avg = np.nanmean(ICallsub_wtn_w, axis=1)\n",
    "    ICallsub_wtn_b_avg = np.nanmean(ICallsub_wtn_b, axis=1)\n",
    "    ICallsub_btw_w_avg = np.nanmean(ICallsub_btw_w, axis=1)\n",
    "    ICallsub_btw_b_avg = np.nanmean(ICallsub_btw_b, axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    data_avg = pd.DataFrame({'degallsub_w_avg' : degallsub_w_avg ,'degallsub_b_avg' : degallsub_b_avg,\n",
    "                       'pet_avg' : pet_avg , 'ICallsub_btw_w_avg' : ICallsub_btw_w_avg , \n",
    "                             'ICallsub_wtn_w_avg' : ICallsub_wtn_w_avg , 'ICallsub_wtn_b_avg' : ICallsub_wtn_b_avg , 'ICallsub_btw_b_avg' : ICallsub_btw_b_avg})\n",
    "        \n",
    "  \n",
    "    return data_avg, ICallsub_w, ICallsub_b, degallsub_w, degallsub_b  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IC_within_btw_network(FCmat,  SCmat, thr_sc, sub_size, scmask, nrois , net_label, net_names):\n",
    "    \n",
    "    num_net = len(net_names)  \n",
    "    SCmat_th = np.zeros((SCmat.shape[0], SCmat.shape[1], sub_size))\n",
    "    for i in range(sub_size):\n",
    "        SCmat_th[:, :, i] = threshold_proportional(SCmat[:, :, i], thr_sc)\n",
    "\n",
    "    SC_mask = SCmat_th.copy()\n",
    "    SC_mask[SC_mask > 0] = 1\n",
    "\n",
    "    if scmask == 1:\n",
    "        FCmat_SC = np.multiply(FCmat , SC_mask)\n",
    "        \n",
    "    else:\n",
    "        FCmat_SC = FCmat\n",
    "\n",
    "    \n",
    "    net_assign_allsub = np.zeros((nrois,num_net, sub_size))\n",
    "    \n",
    "    for i in range(sub_size):\n",
    "        \n",
    "        FC = FCmat_SC[:,:,i]\n",
    "        FC[FC>0] = 1\n",
    "        \n",
    "        net_num = net_label[\"network_number\"]\n",
    "        new_net_num = np.tile(net_num.transpose(), (nrois, 1)) \n",
    "        \n",
    "        FC_net = np.multiply(FC, new_net_num)\n",
    "        \n",
    "        \n",
    "        for r in range(nrois):\n",
    "            for s in range(num_net):\n",
    "                \n",
    "                net_assign_allsub[r,s,i] = np.sum(matrix[r] == s)\n",
    "                \n",
    "    \n",
    "    \n",
    "    entropy_allsub = np.zeros((nrois, sub_size))\n",
    "    \n",
    "    for i in range(sub_size):\n",
    "        net_assign = net_assign_allsub[:,:,i]\n",
    "        \n",
    "        conn_sum = np.sum(net_assign, axis=1)\n",
    "        probabilities = net_assign / conn_sum\n",
    "\n",
    "\n",
    "        entropy_allsub[:,i] = -np.sum(probabilities * np.log2(probabilities))\n",
    "        \n",
    "    \n",
    "    entropy_avg = np.mean(entropy_allsub , axis = 1)\n",
    "    \n",
    "    return entropy_allsub, entropy_avg\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID_calculation(FCmat, pet, SCmat, thr_sc, thr_FC, sub_size, scmask, nrois_rem):\n",
    "    data_single_sub = {\n",
    "        'ICallsub_w': np.zeros((nrois_rem, sub_size)),\n",
    "        'ICallsub_b': np.zeros((nrois_rem, sub_size)),\n",
    "        'degallsub_w': np.zeros((nrois_rem, sub_size)),\n",
    "        'degallsub_b': np.zeros((nrois_rem, sub_size)),\n",
    "        'AvgMIallsub': np.zeros((nrois_rem, sub_size)),\n",
    "        'eig_cenallsub': np.zeros((nrois_rem, sub_size)),\n",
    "        'btw_cenallsub': np.zeros((nrois_rem, sub_size)),\n",
    "    }\n",
    "    \n",
    "    SCmat_th = np.zeros((SCmat.shape[0], SCmat.shape[1], sub_size))\n",
    "    \n",
    "    for i in range(sub_size):\n",
    "        SCmat_th[:, :, i] = threshold_proportional(SCmat[:, :, i], thr_sc)\n",
    "\n",
    "    SC_mask = SCmat_th.copy()\n",
    "    SC_mask[SC_mask > 0] = 1\n",
    "\n",
    "    \n",
    "\n",
    "    pet_rem = pet.copy()\n",
    "\n",
    "    \n",
    "\n",
    "    for j in range(sub_size):\n",
    "        \n",
    "        FCconn = FCmat[:, :, j]\n",
    "        SCconn = SC_mask[:,:,j]\n",
    "        \n",
    "        \n",
    "        \n",
    "        FCconn_th = threshold_proportional(FCconn, thr_FC)\n",
    "        \n",
    "        if scmask == 1:\n",
    "            FCmat_SC = np.multiply(FCconn_th , SCconn)\n",
    "        \n",
    "        else:\n",
    "            FCmat_SC = FCconn_th\n",
    "        CMR2 = pet_rem[:, j]\n",
    "        \n",
    "        #FCconn_th = FCconn\n",
    "        deg_w = np.nansum(FCmat_SC, axis=1)\n",
    "        FCconn_th_b = FCmat_SC.copy()\n",
    "        FCconn_th_b[FCconn_th_b > 0] = 1\n",
    "        deg_b = np.nansum(FCconn_th_b, axis=1)\n",
    "        E_MAT = np.tile(CMR2, (nrois_rem, 1)) #########\n",
    "        \n",
    "        \n",
    "        GFC = nx.Graph(FCmat_SC)\n",
    "        \n",
    "        eigenvector_centrality = nx.eigenvector_centrality(GFC)\n",
    "        eig_cen = list(eigenvector_centrality.values())\n",
    "        \n",
    "        betweenness_centrality = nx.betweenness_centrality(GFC , normalized=True, weight = 'weight')\n",
    "        btw_cen = list(betweenness_centrality.values())\n",
    "\n",
    "        #FCconn_thnor = np.zeros((nrois_rem, nrois_rem))\n",
    "        #FCconn_thnor_b = np.zeros((nrois_rem, nrois_rem))\n",
    "\n",
    "       # for r in range(nrois_rem):\n",
    "          #  for s in range(nrois_rem):\n",
    "              #  FCconn_thnor[r, s] = np.divide(FCconn_th[r, s] , np.nansum(FCconn_th[r, :]))\n",
    "               # FCconn_thnor_b[r, s] = np.divide(FCconn_th_b[r, s]  , np.nansum(FCconn_th_b[r, :]))\n",
    "                \n",
    "        row_sums = np.nansum(FCmat_SC, axis=1)  # Compute the sum of each row\n",
    "        #FCconn_thnor = np.divide(FCmat_SC , row_sums[:, np.newaxis], out=np.zeros_like(FCmat_SC ), where=row_sums[:, np.newaxis] != 0)\n",
    "        \n",
    "        FCconn_thnor = FCmat_SC \n",
    "        row_sums_b = np.nansum(FCconn_th_b, axis=1)  # Compute the sum of each row\n",
    "        FCconn_thnor_b = np.divide(FCconn_th_b, row_sums_b[:, np.newaxis], out=np.zeros_like(FCconn_th_b), where=row_sums[:, np.newaxis] != 0)\n",
    "\n",
    "        IC_w = np.nansum(np.multiply(FCconn_thnor , E_MAT), axis=1)\n",
    "        IC_b = np.nansum(np.multiply(FCconn_thnor_b ,E_MAT), axis=1)\n",
    "        AvgMI = np.divide(deg_w , deg_b)\n",
    "\n",
    "        data_single_sub['ICallsub_w'][:, j] = IC_w\n",
    "        data_single_sub['ICallsub_b'][:, j] = IC_b\n",
    "        data_single_sub['degallsub_w'][:, j] = deg_w\n",
    "        data_single_sub['degallsub_b'][:, j] = deg_b\n",
    "        data_single_sub['AvgMIallsub'][:, j] = AvgMI\n",
    "        data_single_sub['eig_cenallsub'][:, j] = eig_cen\n",
    "        data_single_sub['btw_cenallsub'][:, j] = btw_cen\n",
    "        data_single_sub['petallsub'][:, j] = pet_rem\n",
    "        \n",
    "        \n",
    "\n",
    "    data_single_sub['ICallsub_w'][data_single_sub['ICallsub_w'] == 0] = np.nan\n",
    "    data_single_sub['ICallsub_b'][data_single_sub['ICallsub_b'] == 0] = np.nan\n",
    "    data_single_sub['degallsub_w'][data_single_sub['degallsub_w'] == 0] = np.nan\n",
    "    data_single_sub['degallsub_b'][data_single_sub['degallsub_b'] == 0] = np.nan\n",
    "    data_single_sub['AvgMIallsub'][data_single_sub['AvgMIallsub']== 0] = np.nan\n",
    "    data_single_sub['eig_cenallsub'][data_single_sub['eig_cenallsub'] == 0] = np.nan\n",
    "    data_single_sub['btw_cenallsub'][data_single_sub['btw_cenallsub'] == 0] = np.nan\n",
    "    data_single_sub['petallsub'][data_single_sub['petallsub'] == 0] = np.nan\n",
    "    \n",
    "\n",
    "    degallsub_w_avg = np.nanmean(data_single_sub['degallsub_w'], axis=1)\n",
    "    degallsub_b_avg = np.nanmean(data_single_sub['degallsub_b'], axis=1)\n",
    "    pet_avg = np.nanmean(pet_rem, axis=1)\n",
    "    ICallsub_w_avg = np.nanmean(data_single_sub['ICallsub_w'], axis=1)\n",
    "    ICallsub_b_avg = np.nanmean(data_single_sub['ICallsub_b'], axis=1)\n",
    "    AvgMIallsub_avg = np.nanmean(data_single_sub['AvgMIallsub'], axis=1)\n",
    "    eig_cenallsub_avg = np.nanmean(data_single_sub['eig_cenallsub'], axis=1)\n",
    "    btw_cenallsub_avg = np.nanmean(data_single_sub['btw_cenallsub'], axis=1)\n",
    "    \n",
    "    \n",
    "    data_avg = pd.DataFrame({'degallsub_w_avg' : degallsub_w_avg ,'degallsub_b_avg' : degallsub_b_avg,\n",
    "                       'pet_avg' : pet_avg , 'ICallsub_w_avg' : ICallsub_w_avg , \n",
    "                         'ICallsub_b_avg' : ICallsub_b_avg, 'AvgMIallsub_avg': AvgMIallsub_avg, 'eig_avg': eig_cenallsub_avg, 'btw_avg': btw_cenallsub_avg})\n",
    "        \n",
    "  \n",
    "    return data_avg, data_single_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Spatial_AC(DATA_avg, ic, LIMB,niter):\n",
    "    from scipy import stats\n",
    "\n",
    "    from brainsmash.mapgen.stats import pearsonr\n",
    "    if LIMB == \"without\":\n",
    "        dist_file = \"/RAID1/jupytertmp/mi/input_data/LeftParcelGeodesicDistmat_wolimb.txt\"\n",
    "    else:\n",
    "        dist_file = \"/RAID1/jupytertmp/mi/input_data/LeftParcelGeodesicDistmat.txt\"\n",
    "        \n",
    "\n",
    "    ## ic can be IC or deg_w or deg_b: \n",
    "    \n",
    "            \n",
    "        \n",
    "    ## Contain the limbic network regions for SAC: \n",
    "\n",
    "    # load parcellated neuroimaging maps\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    if ic == \"IC\":\n",
    "        half_size = len(DATA_avg.ICallsub_w_avg.values) // 2  \n",
    "        pet = DATA_avg.pet_avg.values[0:half_size]\n",
    "        IC = DATA_avg.ICallsub_w_avg.values[0:half_size]\n",
    "        \n",
    "        \n",
    "    elif ic == \"deg_w\":\n",
    "        \n",
    "        half_size = len(DATA_avg.ICallsub_w_avg.values) // 2 \n",
    "        IC = DATA_avg.degallsub_w_avg.values[0:half_size]\n",
    "        pet = DATA_avg.pet_avg.values[0:half_size]\n",
    "\n",
    "    elif ic == \"PC\":\n",
    "        \n",
    "        half_size = len(DATA_avg.ICallsub_w_avg.values) // 2 \n",
    "        IC = DATA_avg.PCallsub_w_avg.values[0:half_size]\n",
    "        pet = DATA_avg.pet_avg.values[0:half_size]\n",
    "        \n",
    "    elif ic == \"deg_b\":\n",
    "        \n",
    "        half_size = len(DATA_avg.ICallsub_w_avg.values) // 2 \n",
    "        IC = DATA_avg.degallsub_b_avg.values[0:half_size]\n",
    "        pet = DATA_avg.pet_avg.values[0:half_size]\n",
    "    else:\n",
    "        x_ser = DATA_avg['x']\n",
    "        y_ser = DATA_avg['y']\n",
    "        half_size = len(x_ser) // 2        \n",
    "        IC  = x_ser.to_numpy()[:half_size]\n",
    "        pet = y_ser.to_numpy()[:half_size]\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # instantiate class and generate 1000 surrogates\n",
    "    gen = Base(pet, dist_file) \n",
    "    surrogate_maps = gen(n = niter)\n",
    "\n",
    "\n",
    "    surrogate_brainmap_corrs = pearsonr(IC, surrogate_maps).flatten()\n",
    "    surrogate_pairwise_corrs = pairwise_r(surrogate_maps, flatten=True)\n",
    "\n",
    "    naive_surrogates = np.array([np.random.permutation(pet) for _ in range(niter)])\n",
    "    naive_brainmap_corrs = pearsonr(IC, naive_surrogates).flatten()\n",
    "    naive_pairwise_corrs = pairwise_r(naive_surrogates, flatten=True)\n",
    "    \n",
    "    sac = '#377eb8'  # autocorr-preserving\n",
    "    rc = '#e41a1c'  # randomly shuffled\n",
    "    bins = np.linspace(-1, 1, 51)  # correlation b\n",
    "\n",
    "    # this is the empirical statistic we're creating a null distribution for\n",
    "    test_stat = stats.pearsonr(pet, IC)[0]\n",
    "\n",
    "    base_fit(\n",
    "        x=pet,\n",
    "        D=dist_file,\n",
    "        nsurr= niter,\n",
    "        nh=25,  # these are default kwargs, but shown here for demonstration\n",
    "        deltas=np.arange(0.1, 1, 0.1),\n",
    "        pv=25)  # kwargs are passed to brainsmash.mapgen.base.Base\n",
    "\n",
    "    spatially_naive_p_value = nonparp(test_stat, naive_brainmap_corrs)\n",
    "    sa_corrected_p_value = nonparp(test_stat, surrogate_brainmap_corrs)\n",
    "    \n",
    "\n",
    "\n",
    "    test_stat = stats.pearsonr(pet, IC)[0]\n",
    "    print(\"Pearson correlation:\", test_stat)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"Spatially naive p-value: {:.2e}\".format(spatially_naive_p_value))\n",
    "    print(\"SA-corrected p-value: {:.2e}\".format(sa_corrected_p_value))\n",
    "    \n",
    "    \n",
    "    return test_stat ,surrogate_brainmap_corrs, sa_corrected_p_value, spatially_naive_p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Gaussian copula mutual information estimation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "__version__ = '0.3'\n",
    "\n",
    "def ctransform(x):\n",
    "    \"\"\"Copula transformation (empirical CDF)\n",
    "    cx = ctransform(x) returns the empirical CDF value along the first\n",
    "    axis of x. Data is ranked and scaled within [0 1] (open interval).\n",
    "    \"\"\"\n",
    "\n",
    "    xi = np.argsort(np.atleast_2d(x))\n",
    "    xr = np.argsort(xi)\n",
    "    cx = (xr+1).astype(float) / (xr.shape[-1]+1)\n",
    "    return cx\n",
    " \n",
    "\n",
    "def copnorm(x):\n",
    "    \"\"\"Copula normalization\n",
    "    \n",
    "    cx = copnorm(x) returns standard normal samples with the same empirical\n",
    "    CDF value as the input. Operates along the last axis.\n",
    "    \"\"\"\n",
    "    cx = sp.stats.norm.ppf(ctransform(x))\n",
    "    #cx = sp.special.ndtri(ctransform(x))\n",
    "    return cx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mi_gg(x, y, biascorrect=True, demeaned=False):\n",
    "    \"\"\"Mutual information (MI) between two Gaussian variables in bits\n",
    "   \n",
    "    I = mi_gg(x,y) returns the MI between two (possibly multidimensional)\n",
    "    Gassian variables, x and y, with bias correction.\n",
    "    If x and/or y are multivariate columns must correspond to samples, rows\n",
    "    to dimensions/variables. (Samples last axis) \n",
    "                                                                             \n",
    "    biascorrect : true / false option (default true) which specifies whether\n",
    "    bias correction should be applied to the esimtated MI.\n",
    "    demeaned : false / true option (default false) which specifies whether th\n",
    "    input data already has zero mean (true if it has been copula-normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.atleast_2d(x)\n",
    "    y = np.atleast_2d(y)\n",
    "    if x.ndim > 2 or y.ndim > 2:\n",
    "        raise ValueError(\"x and y must be at most 2d\")\n",
    "    Ntrl = x.shape[1]\n",
    "    Nvarx = x.shape[0]\n",
    "    Nvary = y.shape[0]\n",
    "    Nvarxy = Nvarx+Nvary\n",
    "\n",
    "    if y.shape[1] != Ntrl:\n",
    "        raise ValueError(\"number of trials do not match\")\n",
    "\n",
    "    # joint variable\n",
    "    xy = np.vstack((x,y))\n",
    "    if not demeaned:\n",
    "        xy = xy - xy.mean(axis=1)[:,np.newaxis]\n",
    "    Cxy = np.dot(xy,xy.T) / float(Ntrl - 1)\n",
    "    # submatrices of joint covariance\n",
    "    Cx = Cxy[:Nvarx,:Nvarx]\n",
    "    Cy = Cxy[Nvarx:,Nvarx:]\n",
    "\n",
    "    chCxy = np.linalg.cholesky(Cxy)\n",
    "    chCx = np.linalg.cholesky(Cx)\n",
    "    chCy = np.linalg.cholesky(Cy)\n",
    "\n",
    "    # entropies in nats\n",
    "    # normalizations cancel for mutual information\n",
    "    HX = np.sum(np.log(np.diagonal(chCx))) # + 0.5*Nvarx*(np.log(2*np.pi)+1.0)\n",
    "    HY = np.sum(np.log(np.diagonal(chCy))) # + 0.5*Nvary*(np.log(2*np.pi)+1.0)\n",
    "    HXY = np.sum(np.log(np.diagonal(chCxy))) # + 0.5*Nvarxy*(np.log(2*np.pi)+1.0)\n",
    "\n",
    "    ln2 = np.log(2)\n",
    "    if biascorrect:\n",
    "        psiterms = sp.special.psi((Ntrl - np.arange(1,Nvarxy+1)).astype(np.float)/2.0) / 2.0\n",
    "        dterm = (ln2 - np.log(Ntrl-1.0)) / 2.0\n",
    "        HX = HX - Nvarx*dterm - psiterms[:Nvarx].sum()\n",
    "        HY = HY - Nvary*dterm - psiterms[:Nvary].sum()\n",
    "        HXY = HXY - Nvarxy*dterm - psiterms[:Nvarxy].sum()\n",
    "\n",
    "    # MI in bits\n",
    "    I = (HX + HY - HXY) / ln2\n",
    "    return I\n",
    "\n",
    "\n",
    "\n",
    "def conn_mat_EI_new(data, nvox, nT, Parc1, GM, VOX, num_pcs, ABS, rem_ind):\n",
    "    \n",
    "    if VOX == 1:\n",
    "        \n",
    "        rem_ind = limb_ind;\n",
    "        Parcfmri_gm = np.multiply(Parc1 , GM)\n",
    "        #Parcfmri_gm = Parc1 \n",
    "        Parcfmri_gm_re = np.reshape(Parcfmri_gm, [nvox, 1],order='F')\n",
    "        parcindfmri_gm = np.unique(Parcfmri_gm_re)\n",
    "        parcindfmri_gm = parcindfmri_gm[1:]\n",
    "        nrois_gm = len(np.unique(Parcfmri_gm)) - 1\n",
    "\n",
    "        Parc1_re = np.reshape(Parc1, [nvox, 1],order='F')\n",
    "        nrois = len(np.unique(Parc1_re)) - 1\n",
    "        nrois_rem = nrois - len(rem_ind)\n",
    "        rois_remain = np.arange(1, nrois+1 )\n",
    "        rois_remain[limb_ind ] = 0  \n",
    "        rois_remain = rois_remain[rois_remain != 0] \n",
    "\n",
    "        data_re = np.reshape(data, [nvox, nT] ,order='F')\n",
    "        Parcindfmri = np.unique(Parc1)\n",
    "        Parcindfmri = Parcindfmri[1:]\n",
    "        DATA_avg = np.zeros([nT, nrois])\n",
    "        num_voxels = np.zeros((nrois))\n",
    "        DATA = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(nrois):\n",
    "\n",
    "            if np.any(Parcfmri_gm_re == Parcindfmri[i]):\n",
    "\n",
    "                ind = np.where(Parcfmri_gm_re == Parcindfmri[i])[0]\n",
    "\n",
    "                dd = data_re[ind, :]\n",
    "\n",
    "                # indremove = np.where(np.all(dd == 0, axis=1))[0]\n",
    "                # dd = np.delete(dd, indremove, axis=0)\n",
    "\n",
    "                # if indremove.size == ind.size:\n",
    "                #     zero_rois[i] = 1\n",
    "\n",
    "                DATA[i] = dd\n",
    "                num_voxels[i] = dd.shape[0]\n",
    "                # edata_median[i, 0] = np.median(edata_re[ind])\n",
    "                DATA_avg[:, i] = np.mean(dd, axis=0)\n",
    "            else:\n",
    "                DATA_avg[:, i] = np.nan \n",
    "                DATA[i] = np.zeros((10,1)) \n",
    "\n",
    "        \n",
    "    else:\n",
    "        DATA_avg = data\n",
    "        #nrois = nvox\n",
    "       # nrois_rem = nrois - len(rem_ind)\n",
    "    \n",
    "    \n",
    "\n",
    "    Pearconn = np.corrcoef(DATA_avg, rowvar=False)\n",
    "    Pearconn = np.delete(Pearconn , rem_ind , axis =0)\n",
    "    Pearconn = np.delete(Pearconn , rem_ind , axis =1)\n",
    "\n",
    "    # MVMIconn = np.zeros((nrois_rem, nrois_rem))\n",
    "    MIconn = np.zeros((nrois_rem, nrois_rem))\n",
    "    cDATA_avg = np.transpose(copnorm(np.transpose(DATA_avg)))\n",
    "    np.delete(cDATA_avg, limb_ind)\n",
    "    zero_rois = np.zeros(nrois_rem)\n",
    "    Cdata_abs = {}\n",
    "    Cdata = {}\n",
    "    for i in range(nrois_rem):\n",
    "\n",
    "        X = DATA[ rois_remain[i]-1]\n",
    "        ind = np.where(np.sum(X, axis=0) == 0)[0]\n",
    "        if np.any(X) == True:\n",
    "            X = np.delete(X, ind, axis=0)\n",
    "        else:\n",
    "            zero_rois[i] = 1\n",
    "\n",
    "\n",
    "\n",
    "        quan = np.quantile(num_voxels, [0.25, 0.5, 0.75, 1])\n",
    "        if num_voxels[rois_remain[i]-1] > quan[2]:\n",
    "            num_pcs = 5\n",
    "        elif num_voxels[rois_remain[i]-1] > quan[1]:\n",
    "            num_pcs = 4\n",
    "        elif num_voxels[rois_remain[i]-1] > quan[0]:\n",
    "            num_pcs = 3\n",
    "        else:\n",
    "            num_pcs = 2\n",
    "\n",
    "\n",
    "        pca = PCA()\n",
    "        X_score = pca.fit_transform(np.transpose(X))\n",
    "        CC = copnorm(X_score[:, 0:min(num_pcs, X_score.shape[1])].T)\n",
    "        Cdata_abs[i] = np.hstack((CC, copnorm(np.abs(CC))))\n",
    "        Cdata[i] = CC\n",
    "\n",
    "    for i in range(nrois_rem):\n",
    "\n",
    "        if zero_rois[i]==1:\n",
    "            MIconn[i,:]=np.nan\n",
    "           # MVMIconn[i,:]=np.nan\n",
    "            continue\n",
    "\n",
    "        CX = Cdata[i]\n",
    "        CX2 = Cdata_abs[i]\n",
    "\n",
    "        for j in range(i+1, nrois_rem):\n",
    "\n",
    "            if zero_rois[j]==1:\n",
    "                MIconn[:,j]=np.nan\n",
    "                #MVMIconn[:,j]=np.nan\n",
    "                continue\n",
    "\n",
    "            MIconn[j,i]= mi_gg(cDATA_avg[:,rois_remain[j]-1],cDATA_avg[:,rois_remain[i]-1])  \n",
    "\n",
    "            CY = Cdata[j]     \n",
    "            CY2 = Cdata_abs[j]\n",
    "\n",
    "           # if ABS == 1:\n",
    "               # num = min(CX2.shape[1], CY2.shape[1])\n",
    "               # MVMIconn[i,j] = mi_gg(CX2[:, :num], CY2[:, :num])\n",
    "            #else:\n",
    "               # MVMIconn[i,j] = mi_gg(CX, CY)\n",
    "\n",
    "\n",
    "    MIconn = MIconn + MIconn.T\n",
    "    \n",
    "    \n",
    "            \n",
    "     \n",
    "    #MVMIconn = MVMIconn + MVMIconn.T\n",
    "\n",
    "    return num_voxels  , MIconn , Pearconn\n",
    "\n",
    "\n",
    "def IC_model(X):\n",
    "    E = X[:len(X)//2]\n",
    "    alpha = X[len(X)//2:]\n",
    "    IC = np.nansum(np.multiply(E,alpha))\n",
    "    return IC\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functional connectivity (MIconn , Pearconn, numvox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Functional_conn(DIR, sub, limb_ind, nrois):\n",
    "    \n",
    "    data_input = {}\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    #sub = [3,7,12,14,17,20,23,25,26,28,29,30,31,32,33,35,36,37,38]; \n",
    "    #sub = [3,7,12,14,17,20,23,25,26,28,29,30,31,33,35,36,37,38]; \n",
    "\n",
    "    #subcor = 15 #***with subcortical******\n",
    "    subcor = 0\n",
    "    remain_rois = range(0 , nrois)\n",
    "    remain_rois = np.delete(remain_rois , limb_ind)\n",
    "    nrois_rem = nrois - len(limb_ind)\n",
    "    mask_rois = np.zeros((nrois, len(sub)))\n",
    "    rois_remain = np.zeros((nrois_rem, len(sub)))\n",
    "\n",
    "    data_input['num_voxels'] = np.zeros((nrois, len(sub) ))\n",
    "    data_input['edata_medianallsub'] = np.zeros((nrois_rem, len(sub) ))\n",
    "    data_input['MVMIconn_allsub'] = np.zeros((nrois_rem, nrois_rem, len(sub)))\n",
    "    data_input['MIconn_allsub'] = np.zeros((nrois_rem, nrois_rem, len(sub) ))\n",
    "    data_input['Pearconn_allsub'] = np.zeros((nrois_rem, nrois_rem, len(sub) ))\n",
    "\n",
    "\n",
    "    for b ,subject in enumerate(sub):\n",
    "        \n",
    "        #loading the GM and parcellation in pet and fmri spaces\n",
    "\n",
    "        DIR1 = f'{DIR}sub-{subject:03}/'\n",
    "        Parcfmri =  nib.load(f'{DIR1}MMP_in_func3mm.nii.gz').get_fdata()\n",
    "        #Parcfmri_subcor =  40 * nib.load(DIR1 + 'mmp_subcortical_in_func3mm.nii.gz').get_fdata()\n",
    "        #Parcfmri_subcor_mask =  nib.load(DIR1 + 'mmp_subcortical_in_func3mm_mask.nii.gz').get_fdata()\n",
    "\n",
    "\n",
    "        Parcpet  =  nib.load(DIR1 + 'MMP_in_pet3mm.nii.gz').get_fdata()\n",
    "        #Parcpet_subcor  =  40 * nib.load(DIR1 + 'mmp_subcortical_in_pet3mm.nii.gz').get_fdata()\n",
    "        #Parcpet_subcor_mask =  nib.load(DIR1 + 'mmp_subcortical_in_pet3mm_mask.nii.gz').get_fdata()\n",
    "\n",
    "\n",
    "        GM_fmri  =  nib.load(DIR1 + \"gm_in_func3mm.nii.gz\").get_fdata()\n",
    "        GM_pet   =  nib.load(DIR1 + \"gm_in_pet3mm.nii.gz\").get_fdata()    \n",
    "\n",
    "\n",
    "        #for adding subcortical regions uncomment the 4 following lines:\n",
    "\n",
    "        #indic_func = -Parcfmri_subcor_mask+1\n",
    "        #indic_pet  = -Parcpet_subcor_mask+1\n",
    "        #Parcfmri  =   np.multiply(indic_func,Parcfmri) + Parcfmri_subcor #***with subcortical******\n",
    "        #Parcpet =  np.multiply(indic_pet , Parcpet)  + Parcpet_subcor #***with subcortical******\n",
    "       # GM_fmri =  GM_fmri + Parcfmri_subcor_mask #***with subcortical******\n",
    "        #GM_pet  =  GM_pet   + Parcpet_subcor_mask #***with subcortical******\n",
    "\n",
    "\n",
    "        #loading the fmri and pet data\n",
    "        BDATA = nib.load(DIR1 + 'func3mm.nii.gz').get_fdata()\n",
    "        EDATA = nib.load(DIR1 + 'pet3mm.nii.gz').get_fdata()\n",
    "\n",
    "        v1, v2, v3, v4 = BDATA.shape\n",
    "        nT = v4\n",
    "        nvox = v1 * v2 * v3\n",
    "\n",
    "        v11 , v22 , v33  = EDATA.shape\n",
    "        nvox2 = v11 * v22 * v33\n",
    "\n",
    "\n",
    "\n",
    "        Parcpet_gm = np.multiply(Parcpet , GM_pet) \n",
    "\n",
    "        Parcpet_re = Parcpet_gm.reshape((nvox2, 1),order='F')\n",
    "        parcindpet = np.unique(Parcpet)\n",
    "        parcindpet = parcindpet[1:]\n",
    "        parcindpet_gm = np.unique(Parcpet_re)\n",
    "        parcindpet_gm = parcindpet_gm[1:]\n",
    "        nroispet = len(np.unique(Parcpet_gm)) - 1\n",
    "\n",
    "        # parcellation on Energy data:\n",
    "        edata_re = EDATA.reshape((nvox2, 1),order='F')\n",
    "\n",
    "        for i in range(nrois_rem):\n",
    "\n",
    "            if parcindpet_gm.__contains__(parcindpet[remain_rois[i]]):\n",
    "\n",
    "                indpet = np.where(Parcpet_re == parcindpet[remain_rois[i]])[0]\n",
    "                data_input['edata_medianallsub'][i, b] = np.nanmedian(edata_re[indpet])\n",
    "            else:\n",
    "\n",
    "                data_input['edata_medianallsub'][i, b] = np.nan\n",
    "        [data_input['num_voxels'][:,b],data_input['MIconn_allsub'][:,:,b],data_input['Pearconn_allsub'][:,:,b]] = conn_mat_EI_new(BDATA, nvox, nT, Parcfmri, GM_fmri, 1, 2, 1, limb_ind) \n",
    "        \n",
    "    data_input['num_voxels'] = np.delete(data_input['num_voxels'] , limb_ind, axis = 0)   \n",
    " \n",
    "    return data_input\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structural connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_conn(DIR, sub, nrois):    \n",
    "\n",
    "    \n",
    "    #sub = [3,7,12,14,17,20,23,25,26,28,29,30,31,33,35,36,37,38];  #***with subcortical******\n",
    "    # downloading SC matrices:\n",
    "    SCconn_allsub = np.zeros((nrois, nrois, len(sub)))\n",
    "    for j in range(len(sub)):\n",
    "        subject = sub[j]\n",
    "        DIR1 = os.path.join(DIR, f'sub-{subject:03}')\n",
    "        #sc_dir = os.path.join(DIR1, 'connectom_glasser_subcortical.csv') #***with subcortical******\n",
    "        sc_dir = os.path.join(DIR1, 'scmat.csv')\n",
    "        scmat = pd.read_csv(sc_dir, header=None)\n",
    "        scmat = scmat.iloc[1:, 1:].to_numpy().astype(float)\n",
    "        if scmat.shape[0] < 360:\n",
    "            print(j)\n",
    "            continue\n",
    "        SCconn_allsub[:, :, j] = scmat\n",
    "\n",
    "\n",
    "    SCconn_allsub = np.delete(SCconn_allsub, limb_ind , axis =0)\n",
    "    SCconn_allsub = np.delete(SCconn_allsub , limb_ind, axis =1)\n",
    "    \n",
    "    return SCconn_allsub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding mvMI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Functional_conn_mvmi(DIR, sub, limb_ind, nrois):\n",
    "    \n",
    "    data_input = {}\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "    #sub = [3,7,12,14,17,20,23,25,26,28,29,30,31,32,33,35,36,37,38]; \n",
    "    #sub = [3,7,12,14,17,20,23,25,26,28,29,30,31,33,35,36,37,38]; \n",
    "\n",
    "    #subcor = 15 #***with subcortical******\n",
    "    subcor = 0\n",
    "    remain_rois = range(0 , nrois)\n",
    "    remain_rois = np.delete(remain_rois , limb_ind)\n",
    "    nrois_rem = nrois - len(limb_ind)\n",
    "    mask_rois = np.zeros((nrois, len(sub)))\n",
    "    rois_remain = np.zeros((nrois_rem, len(sub)))\n",
    "\n",
    "    data_input['num_voxels'] = np.zeros((nrois, len(sub) ))\n",
    "    data_input['edata_medianallsub'] = np.zeros((nrois_rem, len(sub) ))\n",
    "    data_input['MVMIconn_allsub'] = np.zeros((nrois_rem, nrois_rem, len(sub)))\n",
    "    data_input['MIconn_allsub'] = np.zeros((nrois_rem, nrois_rem, len(sub) ))\n",
    "    data_input['Pearconn_allsub'] = np.zeros((nrois_rem, nrois_rem, len(sub) ))\n",
    "\n",
    "\n",
    "    for b ,subject in enumerate(sub):\n",
    "        \n",
    "        #loading the GM and parcellation in pet and fmri spaces\n",
    "\n",
    "        DIR1 = f'{DIR}sub-{subject:03}/'\n",
    "        Parcfmri =  nib.load(f'{DIR1}/MMP_in_func3mm.nii.gz').get_fdata()\n",
    "        #Parcfmri_subcor =  40 * nib.load(DIR1 + 'mmp_subcortical_in_func3mm.nii.gz').get_fdata()\n",
    "        #Parcfmri_subcor_mask =  nib.load(DIR1 + 'mmp_subcortical_in_func3mm_mask.nii.gz').get_fdata()\n",
    "\n",
    "\n",
    "        Parcpet  =  nib.load(DIR1 + 'MMP_in_pet3mm.nii.gz').get_fdata()\n",
    "        #Parcpet_subcor  =  40 * nib.load(DIR1 + 'mmp_subcortical_in_pet3mm.nii.gz').get_fdata()\n",
    "        #Parcpet_subcor_mask =  nib.load(DIR1 + 'mmp_subcortical_in_pet3mm_mask.nii.gz').get_fdata()\n",
    "\n",
    "\n",
    "        GM_fmri  =  nib.load(DIR1 + \"gm_in_func3mm.nii.gz\").get_fdata()\n",
    "        GM_pet   =  nib.load(DIR1 + \"gm_in_pet3mm.nii.gz\").get_fdata()    \n",
    "\n",
    "\n",
    "        #for adding subcortical regions uncomment the 4 following lines:\n",
    "\n",
    "        #indic_func = -Parcfmri_subcor_mask+1\n",
    "        #indic_pet  = -Parcpet_subcor_mask+1\n",
    "        #Parcfmri  =   np.multiply(indic_func,Parcfmri) + Parcfmri_subcor #***with subcortical******\n",
    "        #Parcpet =  np.multiply(indic_pet , Parcpet)  + Parcpet_subcor #***with subcortical******\n",
    "       # GM_fmri =  GM_fmri + Parcfmri_subcor_mask #***with subcortical******\n",
    "        #GM_pet  =  GM_pet   + Parcpet_subcor_mask #***with subcortical******\n",
    "\n",
    "\n",
    "        #loading the fmri and pet data\n",
    "        BDATA = nib.load(DIR1 + 'func3mm.nii.gz').get_fdata()\n",
    "        EDATA = nib.load(DIR1 + 'pet3mm.nii.gz').get_fdata()\n",
    "\n",
    "        v1, v2, v3, v4 = BDATA.shape\n",
    "        nT = v4\n",
    "        nvox = v1 * v2 * v3\n",
    "\n",
    "        v11 , v22 , v33  = EDATA.shape\n",
    "        nvox2 = v11 * v22 * v33\n",
    "\n",
    "\n",
    "\n",
    "        Parcpet_gm = np.multiply(Parcpet , GM_pet) \n",
    "\n",
    "        Parcpet_re = Parcpet_gm.reshape((nvox2, 1),order='F')\n",
    "        parcindpet = np.unique(Parcpet)\n",
    "        parcindpet = parcindpet[1:]\n",
    "        parcindpet_gm = np.unique(Parcpet_re)\n",
    "        parcindpet_gm = parcindpet_gm[1:]\n",
    "        nroispet = len(np.unique(Parcpet_gm)) - 1\n",
    "\n",
    "        # parcellation on Energy data:\n",
    "        edata_re = EDATA.reshape((nvox2, 1),order='F')\n",
    "\n",
    "        for i in range(nrois_rem):\n",
    "\n",
    "            if parcindpet_gm.__contains__(parcindpet[remain_rois[i]]):\n",
    "\n",
    "                indpet = np.where(Parcpet_re == parcindpet[remain_rois[i]])[0]\n",
    "                data_input['edata_medianallsub'][i, b] = np.nanmedian(edata_re[indpet])\n",
    "            else:\n",
    "\n",
    "                data_input['edata_medianallsub'][i, b] = np.nan\n",
    "        [data_input['num_voxels'][:,b],data_input['MIconn_allsub'][:,:,b],data_input['Pearconn_allsub'][:,:,b],data_input['MVMIconn_allsub'][:,:,b]] = conn_mat_EI_mvmi(BDATA, nvox, nT, Parcfmri, GM_fmri, 1, 2, 1, limb_ind) \n",
    "        \n",
    "    data_input['num_voxels'] = np.delete(data_input['num_voxels'] , limb_ind, axis = 0)   \n",
    " \n",
    "    return data_input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def conn_mat_EI_mvmi(data, nvox, nT, Parc1, GM, VOX, num_pcs, ABS, rem_ind):\n",
    "    \n",
    "    if VOX == 1:\n",
    "        \n",
    "        rem_ind = limb_ind;\n",
    "        Parcfmri_gm = np.multiply(Parc1 , GM)\n",
    "        Parcfmri_gm_re = np.reshape(Parcfmri_gm, [nvox, 1],order='F')\n",
    "        parcindfmri_gm = np.unique(Parcfmri_gm_re)\n",
    "        parcindfmri_gm = parcindfmri_gm[1:]\n",
    "        nrois_gm = len(np.unique(Parcfmri_gm)) - 1\n",
    "\n",
    "        Parc1_re = np.reshape(Parc1, [nvox, 1],order='F')\n",
    "        nrois = len(np.unique(Parc1_re)) - 1\n",
    "        nrois_rem = nrois - len(rem_ind)\n",
    "        rois_remain = np.arange(1, nrois+1 )\n",
    "        rois_remain[limb_ind ] = 0  \n",
    "        rois_remain = rois_remain[rois_remain != 0] \n",
    "\n",
    "        data_re = np.reshape(data, [nvox, nT] ,order='F')\n",
    "        Parcindfmri = np.unique(Parc1)\n",
    "        Parcindfmri = Parcindfmri[1:]\n",
    "        DATA_avg = np.zeros([nT, nrois])\n",
    "        num_voxels = np.zeros((nrois))\n",
    "        DATA = {}\n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(nrois):\n",
    "\n",
    "            if np.any(Parcfmri_gm_re == Parcindfmri[i]):\n",
    "\n",
    "                ind = np.where(Parcfmri_gm_re == Parcindfmri[i])[0]\n",
    "\n",
    "                dd = data_re[ind, :]\n",
    "\n",
    "                # indremove = np.where(np.all(dd == 0, axis=1))[0]\n",
    "                # dd = np.delete(dd, indremove, axis=0)\n",
    "\n",
    "                # if indremove.size == ind.size:\n",
    "                #     zero_rois[i] = 1\n",
    "\n",
    "                DATA[i] = dd\n",
    "                num_voxels[i] = dd.shape[0]\n",
    "                # edata_median[i, 0] = np.median(edata_re[ind])\n",
    "                DATA_avg[:, i] = np.mean(dd, axis=0)\n",
    "            else:\n",
    "                DATA_avg[:, i] = np.nan \n",
    "                DATA[i] = np.zeros((10,1)) \n",
    "\n",
    "        \n",
    "    else:\n",
    "        DATA_avg = data\n",
    "        #nrois = nvox\n",
    "       # nrois_rem = nrois - len(rem_ind)\n",
    "    \n",
    "    \n",
    "\n",
    "    Pearconn = np.corrcoef(DATA_avg, rowvar=False)\n",
    "    Pearconn = np.delete(Pearconn , rem_ind , axis =0)\n",
    "    Pearconn = np.delete(Pearconn , rem_ind , axis =1)\n",
    "\n",
    "    MVMIconn = np.zeros((nrois_rem, nrois_rem))\n",
    "    MIconn = np.zeros((nrois_rem, nrois_rem))\n",
    "    cDATA_avg = np.transpose(copnorm(np.transpose(DATA_avg)))\n",
    "    np.delete(cDATA_avg, limb_ind)\n",
    "    zero_rois = np.zeros(nrois_rem)\n",
    "    Cdata_abs = {}\n",
    "    Cdata = {}\n",
    "    for i in range(nrois_rem):\n",
    "\n",
    "        X = DATA[ rois_remain[i]-1]\n",
    "        ind = np.where(np.sum(X, axis=0) == 0)[0]\n",
    "        if np.any(X) == True:\n",
    "            X = np.delete(X, ind, axis=0)\n",
    "        else:\n",
    "            zero_rois[i] = 1\n",
    "\n",
    "\n",
    "\n",
    "        quan = np.quantile(num_voxels, [0.25, 0.5, 0.75, 1])\n",
    "        if num_voxels[rois_remain[i]-1] > quan[2]:\n",
    "            num_pcs = 5\n",
    "        elif num_voxels[rois_remain[i]-1] > quan[1]:\n",
    "            num_pcs = 4\n",
    "        elif num_voxels[rois_remain[i]-1] > quan[0]:\n",
    "            num_pcs = 3\n",
    "        else:\n",
    "            num_pcs = 2\n",
    "\n",
    "\n",
    "        pca = PCA()\n",
    "        X_score = pca.fit_transform(np.transpose(X))\n",
    "        CC = copnorm(X_score[:, 0:min(num_pcs, X_score.shape[1])].T)\n",
    "        Cdata_abs[i] = np.hstack((CC, copnorm(np.abs(CC))))\n",
    "        Cdata[i] = CC\n",
    "\n",
    "    for i in range(nrois_rem):\n",
    "\n",
    "        if zero_rois[i]==1:\n",
    "            MIconn[i,:]=np.nan\n",
    "            MVMIconn[i,:]=np.nan\n",
    "            continue\n",
    "\n",
    "        CX = Cdata[i]\n",
    "        CX2 = Cdata_abs[i]\n",
    "\n",
    "        for j in range(i+1, nrois_rem):\n",
    "\n",
    "            if zero_rois[j]==1:\n",
    "                MIconn[:,j]=np.nan\n",
    "                MVMIconn[:,j]=np.nan\n",
    "                continue\n",
    "\n",
    "            MIconn[j,i]= mi_gg(cDATA_avg[:,rois_remain[j]-1],cDATA_avg[:,rois_remain[i]-1])  \n",
    "\n",
    "            CY = Cdata[j]     \n",
    "            CY2 = Cdata_abs[j]\n",
    "\n",
    "            if ABS == 1:\n",
    "                num = min(CX2.shape[1], CY2.shape[1])\n",
    "                MVMIconn[i,j] = mi_gg(CX2[:, :num], CY2[:, :num])\n",
    "            else:\n",
    "                MVMIconn[i,j] = mi_gg(CX, CY)\n",
    "\n",
    "\n",
    "    MIconn = MIconn + MIconn.T\n",
    "    \n",
    "    \n",
    "            \n",
    "     \n",
    "    MVMIconn = MVMIconn + MVMIconn.T\n",
    "\n",
    "    return num_voxels  , MIconn , Pearconn, MVMIconn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steiger_z_test(r12, r13, r23, n):\n",
    "    \"\"\"\n",
    "    Performs Steiger's Z-test to compare two dependent correlation coefficients.\n",
    "\n",
    "    Parameters:\n",
    "        r12 (float): Correlation between variable 1 and 2.\n",
    "        r13 (float): Correlation between variable 1 and 3.\n",
    "        r23 (float): Correlation between variable 2 and 3.\n",
    "        n (int): Sample size.\n",
    "\n",
    "    Returns:\n",
    "        z (float): Z-score.\n",
    "        p (float): Two-tailed p-value.\n",
    "    \"\"\"\n",
    "    # Fisher transformation for r12 and r13\n",
    "    z12 = 0.5 * np.log((1 + r12) / (1 - r12))\n",
    "    z13 = 0.5 * np.log((1 + r13) / (1 - r13))\n",
    "\n",
    "    # Compute standard error\n",
    "    se = np.sqrt((2 * (1 - r23)) / (n - 3))\n",
    "\n",
    "    # Compute Steiger's Z-score\n",
    "    z = (z12 - z13) / se\n",
    "\n",
    "    # Compute two-tailed p-value\n",
    "    p = 2 * (1 - norm.cdf(abs(z)))\n",
    "\n",
    "    return z, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_node_surf(data, conn_mat, param, CMAP, edge_thr, limb_ind):\n",
    " \n",
    "    cmat = np.mean(conn_mat, axis=2)\n",
    "\n",
    "\n",
    "    plotly_cmap = [\n",
    "        [f, '#{:02x}{:02x}{:02x}'.format(*((rgba[:3] * 255).astype(int)))]\n",
    "        for f, rgba in zip(np.linspace(0, 1, 256), CMAP(np.linspace(0, 1, 256)))\n",
    "    ]\n",
    "\n",
    "\n",
    "    IC = data[param].values.astype(float)\n",
    "    IC_min = IC.min()\n",
    "    cap = 34 if param == \"ICallsub_w_avg\" else IC.max()\n",
    "    IC_capped = np.clip(IC, IC_min, cap)\n",
    "    IC_norm = IC\n",
    "    color_values = IC_capped\n",
    "\n",
    "\n",
    "    coords = pd.read_csv(\"/RAID1/jupytertmp/mi/input_data/HCP-MMP1_UniqueRegionList.csv\")\n",
    "    coords = coords[[\"x-cog\", \"y-cog\", \"z-cog\"]]\n",
    "    coords_df = coords.drop(limb_ind, axis=0)\n",
    "    coords_df.columns = coords_df.columns.str.strip()\n",
    "    nodes = coords_df[['x-cog', 'y-cog', 'z-cog']].values\n",
    "\n",
    "   \n",
    "    G = nx.Graph()\n",
    "    for idx, node in enumerate(nodes):\n",
    "        G.add_node(idx, coord=node)\n",
    "    source, target = np.nonzero(np.triu(cmat) > edge_thr)\n",
    "    edges = list(zip(source, target))\n",
    "    G.add_edges_from(edges)\n",
    "\n",
    "    nodes_x = coords_df['x-cog'].values\n",
    "    nodes_y = coords_df['y-cog'].values\n",
    "    nodes_z = coords_df['z-cog'].values\n",
    "\n",
    "    p85, p90, p95 = np.percentile(IC_norm, [85, 90, 95])\n",
    "    node_sizes = np.full_like(IC_norm, 8, dtype=np.float32)\n",
    "    node_sizes[IC_norm >= p85] = 12\n",
    "    node_sizes[IC_norm >= p90] = 17\n",
    "    node_sizes[IC_norm >= p95] = 22\n",
    "\n",
    "    edge_x, edge_y, edge_z = [], [], []\n",
    "    for s, t in edges:\n",
    "        edge_x += [nodes_x[s], nodes_x[t], None]\n",
    "        edge_y += [nodes_y[s], nodes_y[t], None]\n",
    "        edge_z += [nodes_z[s], nodes_z[t], None]\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # --- nodes\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=nodes_x, y=nodes_y, z=nodes_z, mode='markers', name='Nodes',\n",
    "        marker=dict(size=node_sizes, color=color_values, colorscale=plotly_cmap,\n",
    "                    opacity=0.8, showscale=False),\n",
    "        showlegend=False, hoverinfo='text'\n",
    "    ))\n",
    "\n",
    "    # --- edges\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=edge_x, y=edge_y, z=edge_z, mode='lines', name='Edges',\n",
    "        line=dict(color='gray'), opacity=0.1, showlegend=False, hoverinfo='none'\n",
    "    ))\n",
    "\n",
    "\n",
    "    logical_sizes = [8, 12, 17, 22]\n",
    "    labels = [\" 85th pct\", \" 85th pct\", \" 90th pct\", \" 95th pct\"]\n",
    "\n",
    "   \n",
    "    leg_min, leg_max = 6.0, 15.0\n",
    "    s_min, s_max = min(logical_sizes), max(logical_sizes)\n",
    "    legend_sizes = [leg_min + (s - s_min) * (leg_max - leg_min) / (s_max - s_min)\n",
    "                    for s in logical_sizes]\n",
    "\n",
    "    size_levels = np.array([8, 12, 17, 22], dtype=float)   # your original tiers\n",
    "\n",
    "    # pick a global scale so the figure isn't crowded (tweak 0.550.8 to taste)\n",
    "    size_scale = 0.65\n",
    "    size_levels_scaled = (size_levels * size_scale).tolist()\n",
    "    \n",
    "    # thresholds\n",
    "    p85, p90, p95 = np.percentile(IC_norm, [85, 90, 95])\n",
    "    \n",
    "    # apply to nodes (now nodes and legend will be identical in pixels)\n",
    "    node_sizes = np.full_like(IC_norm, size_levels_scaled[0], dtype=float)\n",
    "    node_sizes[IC_norm >= p85] = size_levels_scaled[1]\n",
    "    node_sizes[IC_norm >= p90] = size_levels_scaled[2]\n",
    "    node_sizes[IC_norm >= p95] = size_levels_scaled[3]\n",
    "\n",
    "\n",
    "  \n",
    "    for label, s in zip(labels, legend_sizes):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[None], y=[None], mode=\"markers\", name=label, hoverinfo=\"skip\", showlegend=True,\n",
    "            marker=dict(size=s, color=\"rgba(128,128,128,0.9)\", line=dict(color=\"gray\", width=1)),\n",
    "            legendgroup=\"size\",\n",
    "        ))\n",
    "\n",
    "    \n",
    "    # --- orientation, background, and tight legend placement ---\n",
    "    fig.update_layout(\n",
    "        # fill the canvas; leave a slim band for the legend\n",
    "        scene=dict(\n",
    "            bgcolor=\"white\",\n",
    "            xaxis=dict(visible=False),\n",
    "            yaxis=dict(visible=False),\n",
    "            zaxis=dict(visible=False),\n",
    "            aspectmode=\"data\",\n",
    "            domain=dict(y=[0.12, 1.0])   # push the 3D scene up; ~12% space for legend\n",
    "        ),\n",
    "        scene_camera=dict(\n",
    "            eye=dict(x=0.0, y=1.8, z=0.08),  # frontal view; smaller values = more zoom\n",
    "            up=dict(x=0, y=0, z=1),\n",
    "            center=dict(x=0, y=0, z=0),\n",
    "        ),\n",
    "    \n",
    "        # legend right below the scene, centered\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            itemsizing=\"trace\",\n",
    "            x=0.5, xanchor=\"center\",\n",
    "            y=0.15, yanchor=\"top\"          # sits just under the scene\n",
    "        ),\n",
    "    \n",
    "        # remove blue page background\n",
    "        paper_bgcolor=\"white\",\n",
    "        plot_bgcolor=\"white\",\n",
    "    \n",
    "        width=900, height=900,\n",
    "        margin=dict(l=0, r=0, t=0, b=0),\n",
    "    \n",
    "        # hide the 2D axes created by legend dummies\n",
    "        xaxis=dict(visible=False),\n",
    "        yaxis=dict(visible=False),\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # --- save (requires 'kaleido'); comment out if not installed\n",
    "    # fig.write_image(f'Figures/node_surface_{param}.png', scale=3)\n",
    "    fig.show()\n",
    "\n",
    "    print(\"Original number of regions:\", coords.shape[0])\n",
    "    print(\"Removed limb regions:\", len(limb_ind))\n",
    "    print(\"Remaining plotted regions:\", len(nodes_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ## edge_thr is not a proportional thresholding, but is a normal thresholidng>> larger less connections. \n",
    "# ## cmat is the matrix contains all subjects, connectivity matrices. \n",
    "# def plot_node_surf(data, conn_mat, param, CMAP,edge_thr, limb_ind):    \n",
    "\n",
    "#        #******************************************************\n",
    "#     # degree:\n",
    "#     cmat = np.mean(conn_mat, axis = 2)\n",
    " \n",
    "    \n",
    "#     plotly_cmap = [\n",
    "#     [f,\n",
    "#      '#{:02x}{:02x}{:02x}'.format(*((rgba[:3] * 255).astype(int)))]\n",
    "#     for f, rgba in zip(\n",
    "#         np.linspace(0, 1, 256),\n",
    "#         CMAP(np.linspace(0, 1, 256)))]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#     #IC:\n",
    "#     # cmat = np.mean(MIconn_allsub, axis = 2)\n",
    "#     # IC = DATA_avg.ICallsub_w_avg\n",
    "#     # edge_thr = 0.1\n",
    "#     #CMAP = 'viridis'\n",
    "\n",
    "\n",
    "#     IC = data[param].values.astype(float)\n",
    "#     IC_min = IC.min()\n",
    "    \n",
    "#     cap = 34 if param==\"ICallsub_w_avg\" else IC.max()\n",
    "    \n",
    "#     # clamp\n",
    "#     IC_capped = np.clip(IC, IC_min, cap)\n",
    "    \n",
    "#     # normalize into [0,1]\n",
    "#     #IC_norm = (IC_capped - IC_min) / (cap - IC_min)\n",
    "#     IC_norm = IC\n",
    "#     color_values = IC_capped\n",
    "    \n",
    "#     coords = pd.read_csv(\"/RAID1/jupytertmp/mi/input_data/HCP-MMP1_UniqueRegionList.csv\")\n",
    "#     coords = coords[[\"x-cog\", \"y-cog\", \"z-cog\"]]\n",
    "#     coords_mat = coords.drop( limb_ind , axis = 0)\n",
    "#     coords_df = coords_mat\n",
    "#     coords_df.columns = coords_df.columns.str.strip()  \n",
    "#     nodes = coords_df[['x-cog', 'y-cog', 'z-cog']].values  \n",
    "    \n",
    "    \n",
    "#     # Create graph and nodes\n",
    "#     G = nx.Graph()\n",
    "#     for idx, node in enumerate(nodes):\n",
    "#         G.add_node(idx, coord=node)\n",
    "    \n",
    "#     # Extract edges based on connectivity threshold\n",
    "#     [source, target] = np.nonzero(np.triu(cmat) > edge_thr)  \n",
    "#     edges = list(zip(source, target))\n",
    "#     G.add_edges_from(edges)\n",
    "    \n",
    "#     # Get node coordinates from DataFrame\n",
    "#     nodes_x = coords_df['x-cog'].values\n",
    "#     nodes_y = coords_df['y-cog'].values\n",
    "#     nodes_z = coords_df['z-cog'].values\n",
    "    \n",
    "#     percentiles = np.percentile(IC_norm, [85, 90, 95])  # 85th, 90th, 95th percentiles\n",
    "#     node_sizes = np.full_like(IC_norm, 8, dtype=np.float32)  # Default smallest size\n",
    "#     node_sizes[IC_norm >= percentiles[0]] = 12  # 15% largest nodes\n",
    "#     node_sizes[IC_norm >= percentiles[1]] = 17  # 10% largest nodes\n",
    "#     node_sizes[IC_norm >= percentiles[2]] = 22  # 5% largest nodes\n",
    "    \n",
    "    \n",
    "#     # Get edge coordinates\n",
    "#     edge_x, edge_y, edge_z = [], [], []\n",
    "#     for s, t in edges:\n",
    "#         edge_x += [nodes_x[s], nodes_x[t], None]\n",
    "#         edge_y += [nodes_y[s], nodes_y[t], None]\n",
    "#         edge_z += [nodes_z[s], nodes_z[t], None]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Plot\n",
    "#     fig = go.Figure()\n",
    "    \n",
    "#     # Nodes (size based on IC values)\n",
    "#     fig.add_trace(go.Scatter3d(\n",
    "#         x=nodes_x, y=nodes_y, z=nodes_z,\n",
    "#         mode='markers', hoverinfo='text', name='Nodes',\n",
    "#         marker=dict(\n",
    "#             size=node_sizes,  # Use computed sizes\n",
    "#             color = color_values,  # Node color intensity based on IC values\n",
    "#             colorscale=plotly_cmap,  # Choose colormap\n",
    "#             opacity=0.8,\n",
    "#             showscale=False,\n",
    "#             colorbar=dict(title=\"IC Intensity\")  # Show color scale legend\n",
    "#         ),\n",
    "#         showlegend=False\n",
    "#     ))\n",
    "    \n",
    "    \n",
    "#     # Edges\n",
    "#     fig.add_trace(go.Scatter3d(\n",
    "#         x=edge_x, y=edge_y, z=edge_z,\n",
    "#         mode='lines', hoverinfo='none', name='Edges',\n",
    "#         opacity=0.1, line=dict(color='gray'), showlegend = False\n",
    "#     ))\n",
    "    \n",
    "#     # Layout\n",
    "#     fig.update_layout(\n",
    "#         scene=dict(\n",
    "#             xaxis=dict(showticklabels=False, visible=False),\n",
    "#             yaxis=dict(showticklabels=False, visible=False),\n",
    "#             zaxis=dict(showticklabels=False, visible=False),\n",
    "#         ),\n",
    "#         width=800, height=600\n",
    "#     )\n",
    "\n",
    "    \n",
    "#     p85, p90, p95 = np.percentile(IC_norm, [85, 90, 95])\n",
    "#    # ---- after you add your node & edge traces ----\n",
    "\n",
    "    \n",
    "#     size_legend = [\n",
    "#         (\" 85th pct\", 8),\n",
    "#         (\" 85th pct\", 12),\n",
    "#         (\" 90th pct\", 17),\n",
    "#         (\" 95th pct\", 22),   # match your node_sizes thresholds\n",
    "#     ]\n",
    "    \n",
    "#     # Use 2D scatter for legend icons so sizes are honored\n",
    "#     for label, s in size_legend:\n",
    "#         fig.add_trace(\n",
    "#             go.Scatter(\n",
    "#                 x=[None], y=[None],\n",
    "#                 mode=\"markers\", name=label, hoverinfo=\"skip\", showlegend=True,\n",
    "#                 marker=dict(\n",
    "#                     size=s,\n",
    "#                     color=\"rgba(128,128,128,0.9)\",\n",
    "#                     line=dict(color=\"gray\", width=1)\n",
    "#                 ),\n",
    "#                 legendgroup=\"size\",\n",
    "#             )\n",
    "#         )\n",
    "    \n",
    "\n",
    "#     fig.update_layout(\n",
    "#         scene=dict(\n",
    "#             xaxis=dict(showticklabels=False, visible=False),\n",
    "#             yaxis=dict(showticklabels=False, visible=False),\n",
    "#             zaxis=dict(showticklabels=False, visible=False),\n",
    "#             aspectmode=\"data\",\n",
    "#         ),\n",
    "#         scene_camera=dict(\n",
    "#             eye=dict(x=0.0, y=2.6, z=0.12),    #  frontal view; increase/decrease to zoom\n",
    "#             up=dict(x=0, y=0, z=1),\n",
    "#         ),\n",
    "#         legend=dict(\n",
    "#             orientation=\"h\",\n",
    "#             itemsizing=\"trace\",                 #  makes legend sizes reflect marker.size\n",
    "#             yanchor=\"bottom\", y=-0.08,\n",
    "#             xanchor=\"center\",  x=0.5,\n",
    "#         ),\n",
    "#         width=900, height=900,\n",
    "#         margin=dict(l=0, r=0, t=0, b=0),\n",
    "#     )\n",
    "    \n",
    "#     plt.savefig(f'Figures/node_surface_{param}.png', dpi=300, bbox_inches='tight')\n",
    "#     fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # IC = IC.reshape(-1, 1)\n",
    "#     # IC_allrois = np.zeros((360,1))\n",
    "#     # ind = np.arange(360)\n",
    "#     # mask = np.ones(ind.shape , dtype = bool)\n",
    "#     # mask[limb_ind] = False\n",
    "#     # ind = ind[mask]\n",
    "#     # IC_allrois[ind] = np.abs(IC)\n",
    "    \n",
    "    \n",
    "#     # fig = plt.figure(figsize=(5, 5),dpi=300)\n",
    "    \n",
    "#     # fsaverage = datasets.fetch_surf_fsaverage()\n",
    "    \n",
    "#     # V_min = IC.min()\n",
    "#     # V_max = cap\n",
    "    \n",
    "#     # if param == \"ICallsub_w_avg\":\n",
    "        \n",
    "#     #     V_max = 34\n",
    "    \n",
    "#     # col_fsa = parcel_to_surface(IC_allrois,'glasser_360_fsa5') \n",
    "#     # plotting.plot_surf_roi(fsaverage['infl_left'], roi_map = col_fsa[:int(col_fsa.shape[0]/2)],\n",
    "#     #                            title='IC, left hemisphere',\n",
    "#     #                            hemi='left', vmin= V_min, vmax = V_max, view='lateral',\n",
    "#     #                            bg_map=fsaverage['sulc_left'], bg_on_data=False,\n",
    "#     #                            darkness=0.5, cmap=CMAP,colorbar=False)\n",
    "#     # plt.show()\n",
    "    \n",
    "#     # col_fsa = parcel_to_surface(IC_allrois,'glasser_360_fsa5') \n",
    "#     # plotting.plot_surf_roi(fsaverage['infl_left'], roi_map = col_fsa[:int(col_fsa.shape[0]/2)],\n",
    "#     #                            title='IC, left hemisphere',\n",
    "#     #                            hemi='left', vmin= V_min, vmax = V_max, view='medial',\n",
    "#     #                            bg_map=fsaverage['sulc_left'], bg_on_data = False,\n",
    "#     #                            darkness=0.5,cmap=CMAP,colorbar=False)\n",
    "#     # plt.show()\n",
    "    \n",
    "    \n",
    "#     # col_fsa = parcel_to_surface(IC_allrois,'glasser_360_fsa5') \n",
    "#     # plotting.plot_surf_roi(fsaverage['infl_right'], roi_map = col_fsa[int(col_fsa.shape[0]/2):],\n",
    "#     #                            title='IC, right hemisphere',\n",
    "#     #                            hemi='right', vmin= V_min, vmax = V_max, view='lateral',\n",
    "#     #                            bg_map=fsaverage['sulc_right'], bg_on_data=False,\n",
    "#     #                            darkness=0.5, cmap=CMAP,colorbar=False)\n",
    "#     # plt.show()\n",
    "    \n",
    "#     # col_fsa = parcel_to_surface(IC_allrois,'glasser_360_fsa5') \n",
    "#     # plotting.plot_surf_roi(fsaverage['infl_right'], roi_map = col_fsa[int(col_fsa.shape[0]/2):],\n",
    "#     #                            title='IC, right hemisphere',\n",
    "#     #                            hemi='right', vmin= V_min, vmax = V_max, view='medial',\n",
    "#     #                            bg_map=fsaverage['sulc_right'], bg_on_data = False,\n",
    "#     #                            darkness=0.5,cmap=CMAP,colorbar=False)\n",
    "#     # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#     # ticks = [V_min,  (V_min+V_max)/2 ,V_max]\n",
    "#     # norm = mpl.colors.Normalize(vmin=V_min, vmax=V_max)\n",
    "#     # sm = mpl.cm.ScalarMappable(cmap=CMAP, norm=norm)\n",
    "#     # sm.set_array([])\n",
    "    \n",
    "#     # fig, ax = plt.subplots(figsize=(6, 1))\n",
    "    \n",
    "#     # cbar = plt.colorbar(\n",
    "#     #     sm,\n",
    "#     #     cax         = ax,\n",
    "#     #     orientation = 'horizontal',\n",
    "#     #     ticks       = ticks,\n",
    "#     #     format      = \"%.2f\"\n",
    "#     # )\n",
    "#     # cbar.set_label(\"Participation Coefficient\", fontsize=10)\n",
    "#     # plt.tight_layout()\n",
    "#     # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GL neuroenergeticslab2024 py3.11",
   "language": "python",
   "name": "neuroenergeticslab2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
